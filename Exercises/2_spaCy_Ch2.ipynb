{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Large-scale data analysis with Spacy (by. huffon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 챕터에서는 큰 용량의 데이터에서 원하는 정보를 추출하는 방법을 배워볼 것입니다.\n",
    "- 해당 튜토리얼을 통해 `spaCy` 라이브러리의 자료구조들을 생성하는 법과, \n",
    "- 어떻게 하면 효과적으로 통계 기반과 규칙 기반 분석 방법을 결합할 수 있을지에 대하여 학습하게 될 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Structures (1): Vocab, Lexemes and StringStore\n",
    "#### Shared vocab and string store (1)\n",
    "- `Vocab`: doc 객체들 간 공유하는 단어를 저장하는 자료구조\n",
    "- 메모리를 절약하기 위해, spaCy는 모든 문자열을 해시 값으로 인코드\n",
    "- 문자열은 `nip.vocab.strings`를 통해 `StringStore`에 단 한 번 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value:  3197928453018144401\n",
      "string value:  coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value: ', nlp.vocab.strings['coffee'])\n",
    "print('string value: ', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value: ', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `lexeme`는 vocab의 최소 단위 객체로 문맥에 독립적인 단어 정보들을 포함\n",
    "    - 단어 정보의 예: `lexeme.text`와 `lexeme.orth` (해시 값) \n",
    "    - `lexeme.is_alpha`와 같은 어휘 속성 역시 포함\n",
    "\n",
    "<br/>\n",
    "<img src='https://course.spacy.io/vocab_stringstore.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print('{} / {} / {}'.format(lexeme.text, lexeme.orth, lexeme.is_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Strings to hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해시 값을 얻기 위해 문자열 **PERSON**을 `nlp.vocab.strings`에 대입해봅시다.\n",
    "- 문자열을 얻기 위해 **PERSON**의 해시 값을 다시 대입해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14478711426843143707 15413499603803372127\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"HHHHHHHHHH HHHH\")\n",
    "print(nlp.vocab.strings[\"HHHHHHHHHH\"],nlp.vocab.strings[\"HHHH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy는 기존 vocab에 없는 단어가 주어질 경우 이를 자동으로 추가 저장함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Structures (2): Doc, Span and Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Doc 객체 임포트\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 단어 리스트와 공백 리스트 정의\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# 앞서 정의한 리스트들을 이용해 doc 객체 수동으로 생성\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span object\n",
    "\n",
    "<img src='https://course.spacy.io/span_indices.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "# Doc과 Span 클래스 임포트\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# 단어 리스트와 공백 리스트 정의\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# 위에서 했던 것과 같이 Doc 객체 수동으로 정의\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Doc 객체 이용해 Span 객체를 수동으로 정의\n",
    "span = Span(doc, 0, 2)\n",
    "print(span)\n",
    "\n",
    "# Span 객체에 라벨 부여\n",
    "span_with_label = Span(doc, 0, 2, label=\"Greeting\")\n",
    "\n",
    "# doc 객체의 엔티티로 생성한 Span 추가\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world Greeting\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3474706295102377020"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['Coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Doc`과 `Span` 은 매우 강력한 객체입니다.\n",
    "    - 해당 객체들로 오래오래 사용하고, 최대한 나중에 string으로 변환합시다 !\n",
    "- Token 객체의 속성을 최대한 활용하세요 ! (e.g. token.i를 사용해 index 추출 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc 객체에서 사용된 단어들의 저장을 위해 vocab을 Doc에 넘겨주는 것을 잊지마세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 목표 텍스트: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# 단어, 공백 리스트 + vocab 이용해 Doc 객체 수동 생성\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"David Bowie\"를 나타내는 `span` 객체를 생성 후, `\"PERSON\"` 라벨을 부여하세요\n",
    "- \"David Bowie\" 를 나타내는 `span` 객체를 doc.ents에 덮어쓰세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# 단어, 공백 리스트 + vocab 이용해 Doc 객체 수동 생성\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# \"David Bowie\"를 커버하는 Span 객체를 doc으로부터 생성 후, \"PERSON\" 라벨 부여\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, '/', span.label_)\n",
    "\n",
    "# Span 객체를 doc 엔티티에 추가\n",
    "doc.ents = [span]\n",
    "\n",
    "# 엔티티의 텍스트와 라벨 속성을 출력\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word vectors and semantic similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spaCy`는 두 객체를 비교해 **유사도** 예측 가능\n",
    "- `Doc.similarity()`, `Span.similarity()`, `Token.similarity()`\n",
    "- 위 메소드들은 인자로 다른 객체를 받으며, `0-1` 사이의 유사도를 반환\n",
    "- **주의**: word vector를 포함하고 있는 크기의 모델을 다운로드 받아야 함\n",
    "    - `en_core_web_md` 혹은 `en_core_web_lg`\n",
    "    - Small model은 word vector 포함하지 않음 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
      "\u001b[K     |████████████████████████████████| 95.4MB 3.4MB/s eta 0:00:01    |█▊                              | 5.2MB 557kB/s eta 0:02:42     |████▊                           | 14.0MB 2.0MB/s eta 0:00:42     |███████▊                        | 23.0MB 1.4MB/s eta 0:00:54     |████████▋                       | 25.7MB 2.3MB/s eta 0:00:30     |█████████▏                      | 27.3MB 1.1MB/s eta 0:01:05     |███████████▎                    | 33.6MB 1.0MB/s eta 0:01:01ta 0:00:45     |███████████████▎                | 45.5MB 1.5MB/s eta 0:00:34     |████████████████                | 47.5MB 3.7MB/s eta 0:00:14     |████████████████▏               | 48.2MB 3.7MB/s eta 0:00:13     |████████████████▏               | 48.4MB 3.7MB/s eta 0:00:13     |█████████████████▍              | 51.9MB 984kB/s eta 0:00:45     |███████████████████████▉        | 71.2MB 2.4MB/s eta 0:00:11     |███████████████████████████▋    | 82.4MB 1.6MB/s eta 0:00:08     |█████████████████████████████▎  | 87.5MB 449kB/s eta 0:00:18     |███████████████████████████████▎| 93.2MB 1.4MB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/xk/bpft64nx4cq4tx5dp3w5czm80000gn/T/pip-ephem-wheel-cache-5rhgcejy/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vector를 포함하는 크기의 모델 로드\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627203210548107\n"
     ]
    }
   ],
   "source": [
    "# 두 개의 객체를 비교해 유사도 추출\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 document의 vector\n",
    "a, b = doc1.vector, doc2.vector\n",
    "\n",
    "# 각 document의 token별 vector의 평균\n",
    "c, d = sum(vec.vector for vec in doc1)/len(doc1), sum(vec.vector for vec in doc2)/len(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((a-c)**2),sum((b-d)**2) # a = c, b = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n$차원 벡터 $a$, $b$ 에 대하여 \n",
    "\n",
    "$\n",
    "\\cos(a,b) = \\cos(\\theta) = \\frac{a\\cdot b}{\\|a\\| \\cdot \\|b\\|}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(a,b) =  0.8627204\n"
     ]
    }
   ],
   "source": [
    "a_dot_b = (a*b).sum()\n",
    "abs_a = np.sqrt((a**2).sum())\n",
    "abs_b = np.sqrt((b**2).sum())\n",
    "\n",
    "print('cos(a,b) = ', a_dot_b/(abs_a * abs_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy에서 document의 word vector는 각 token별 word vector의 평균으로 정의되며,<br>\n",
    "similarity는 word vector들 사이의 cosine similarity 로 정의됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.55549127\n",
      "I pizza 0.29188403\n",
      "like I 0.55549127\n",
      "like like 1.0\n",
      "like pizza 0.3342793\n",
      "fast I 0.36169332\n",
      "fast like 0.43584064\n",
      "fast pizza 0.29867014\n",
      "food I 0.29509223\n",
      "food like 0.3927976\n",
      "food pizza 0.59247416\n"
     ]
    }
   ],
   "source": [
    "for vec1 in doc1:\n",
    "    for  vec2 in doc2:\n",
    "        print(vec1, vec2, vec1.similaraity(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "like like 1.0\n",
      "fast pizza 0.29867014\n"
     ]
    }
   ],
   "source": [
    "for vec1, vec2 in zip(doc1, doc2):\n",
    "    print(vec1, vec2, vec1.similarity(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = doc1.vector\n",
    "b = doc2.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627204"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pizza와 pasta 토큰 비교\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token, Doc, Span 등 서로 다른 객체 간 비교도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc과 Token의 비교\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199090950699994\n"
     ]
    }
   ],
   "source": [
    "# Span과 Doc의 비교\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(a,b) =  0.6199092\n"
     ]
    }
   ],
   "source": [
    "a, b = span.vector, doc.vector\n",
    "a_dot_b = (a*b).sum()\n",
    "abs_a = np.sqrt((a**2).sum())\n",
    "abs_b = np.sqrt((b**2).sum())\n",
    "\n",
    "print('cos(a,b) = ', a_dot_b/(abs_a * abs_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great restaurant / really nice bar\n",
      "0.75173926\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# \"great restaurant\" Span과 \"really nice bar\" Span 정의\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "print(span1, '/', span2)\n",
    "\n",
    "# Span 간 유사도 계산\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I love pasta')\n",
    "tt = doc.vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sum(vec.vector for vec in doc)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt-ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy는 어떻게 유사도를 측정하고 있을까요?\n",
    "- 유사도는 **Word vectors**를 이용해 결정됩니다.\n",
    "- 이는 단어들의 의미를 다차원 공간에 나타낸 수치입니다.\n",
    "- Word vector는 **Word2Vec**을 비롯한 다양한 알고리즘을 이용해 생성할 수 있습니다.\n",
    "- 또한 외부 알고리즘을 통해 구한 유사도를 spaCy의 통계 모델에 추가할 수도 있습니다.\n",
    "- spaCy에서 유사도를 구하는 기본 척도는 **코사인 유사도**이지만, 이는 변경이 가능합니다.\n",
    "- `Doc`과 `Span` 객체의 유사도는 토큰 유사도의 평균으로 정의됩니다.\n",
    "- 긴 문장보다 작은 문장의 유사도가 더 정확히 구해지는데, 이는 문장이 길어질수록 유사도를 구하는데 방해가 되는 단어들이 많아지기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "# token.vector 속성 통해 word vector 출력 가능\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도의 정의는 사용되는 어플리케이션에 따라 다를 수 있음\n",
    "- 문맥과 어플리케이션의 목적에 따라 유사도의 정의가 달라져야 함 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "# Is it similar or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Combining models and rules\n",
    "- 통계 모델은 일반화 된 분석 결과를 반환해주지만, 모든 경우를 포함할 수는 없기에 규칙 기반의 시스템과 결합되어 사용되는 것이 바람직함\n",
    "- `spaCy`에서는 `Matcher`, `PhraseMatcher` 클래스 등을 이용해 외부 규칙들을 더해줄 수 있음\n",
    "- Matcher 클래스 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 효과적인 구(Phrase) 매칭 방법\n",
    "- `PhraseMatcher`를 사용하면 보다 효율적으로 **구**를 매칭할 수 있음\n",
    "- 이때 `Doc` 객체를 패턴으로 취함\n",
    "- `Matcher` 클래스보다 빠르고 효과적 !\n",
    "    - 때로는 개별 토큰을 추출하는 패턴을 작성하는 것보다 문자열 그대로를 매칭하는  `PhraseMatcher`을 사용하는 것이 더 효과적일 때가 있음\n",
    "        - e.g.) 국가명과 같은 유한한 범주에서 문자열을 추출하고자 할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\") # Doc -> pattern\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# 매칭 결과를 순회\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 매칭 결과를 Span 객체로 생성\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
