{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 연습\n",
    "\n",
    "PTB 데이터셋을 이용하여 word2vec(skip-gram) 알고리즘을 구현해보는 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 PTB 데이터셋은 [여기](https://github.com/tomsercu/lstm)에서 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = open('.\\data\\ptb.train.txt').read().replace('\\n','<eos>').strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has',\n",
       " 'been',\n",
       " 'less',\n",
       " 'prominent',\n",
       " 'according',\n",
       " 'to',\n",
       " 'mr.',\n",
       " '<unk>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words[929580:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in train_words:\n",
    "    if word not in word_to_id:\n",
    "        tmp_id = len(word_to_id)\n",
    "        word_to_id[word]= tmp_id\n",
    "        id_to_word[tmp_id]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNIT = 100\n",
    "WINDOW_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = []\n",
    "for i in range(WINDOW_SIZE, len(train_words)-WINDOW_SIZE):\n",
    "    data = [train_words[i], [train_words[i-j-1] for j in range(WINDOW_SIZE)]+\n",
    "                             [train_words[i+j+1] for j in range(WINDOW_SIZE) ]]\n",
    "    n_gram.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calloway', ['berlitz', 'banknote', 'aer', 'centrust', 'cluett', 'fromstein']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, V = torch.randn((len(word_to_id),HIDDEN_UNIT)),torch.randn(HIDDEN_UNIT,len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'energy'\n",
    "word_to_id[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = W[word_to_id[word]].view(1,-1)\n",
    "h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10000])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(h,V).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec(nn.Module):\n",
    "    \"\"\"\n",
    "        데이터를 받으면 \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,data=n_gram, vocab=vocab, window_size=WINDOW_SIZE,\n",
    "                 word_to_id=word_to_id, id_to_word=id_to_word, hidden_unit=HIDDEN_UNIT):\n",
    "        super(Word2vec, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.size = len(vocab)\n",
    "        self.hidden = hidden_unit\n",
    "        self.W1 = torch.nn.Linear(len(self.word_to_id),self.hidden, bias=False)\n",
    "        self.W2 = torch.nn.Linear(self.hidden,len(self.word_to_id), bias=False)\n",
    "\n",
    "\n",
    "        self.window_size = WINDOW_SIZE\n",
    "        self.softmax = []\n",
    "        for i in range(2* self.window_size):\n",
    "            self.softmax.append(nn.LogSoftmax())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.zeros((2* self.window_size, self.size))\n",
    "        \n",
    "        if type(x) == str:\n",
    "            x = self.word_to_id[x]\n",
    "            \n",
    "        z = torch.zeros((1,len(self.vocab)))\n",
    "        z[0,x] = 1\n",
    "            \n",
    "        for i in range(self.window_size):\n",
    "            y[i] = self.softmax[i](self.W2(self.W1(z)))\n",
    "            y[self.window_size+i] = self.softmax[self.window_size+i](self.W2(self.W1(z)))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2vec(\n",
       "  (W1): Linear(in_features=10000, out_features=100, bias=False)\n",
       "  (W2): Linear(in_features=100, out_features=10000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.1663, -9.1202, -9.2288,  ..., -9.1777, -9.2252, -9.2213],\n",
       "        [-9.1663, -9.1202, -9.2288,  ..., -9.1777, -9.2252, -9.2213],\n",
       "        [-9.1663, -9.1202, -9.2288,  ..., -9.1777, -9.2252, -9.2213],\n",
       "        [-9.1663, -9.1202, -9.2288,  ..., -9.1777, -9.2252, -9.2213],\n",
       "        [-9.1663, -9.1202, -9.2288,  ..., -9.1777, -9.2252, -9.2213],\n",
       "        [-9.1663, -9.1202, -9.2288,  ..., -9.1777, -9.2252, -9.2213]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec('next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('W1.weight', Parameter containing:\n",
      "tensor([[-0.0006, -0.0045, -0.0035,  ..., -0.0043, -0.0015, -0.0034],\n",
      "        [ 0.0002, -0.0041, -0.0044,  ...,  0.0096,  0.0016,  0.0029],\n",
      "        [-0.0076,  0.0055,  0.0022,  ...,  0.0010, -0.0042,  0.0065],\n",
      "        ...,\n",
      "        [ 0.0019,  0.0001, -0.0017,  ...,  0.0044, -0.0033, -0.0054],\n",
      "        [-0.0001,  0.0016,  0.0011,  ...,  0.0098,  0.0039,  0.0057],\n",
      "        [-0.0041, -0.0096,  0.0030,  ..., -0.0050,  0.0002, -0.0075]],\n",
      "       requires_grad=True))\n",
      "('W1.bias', Parameter containing:\n",
      "tensor([-0.0018, -0.0013,  0.0043,  0.0060,  0.0007,  0.0014, -0.0052, -0.0089,\n",
      "        -0.0006,  0.0064, -0.0036,  0.0095, -0.0070,  0.0041,  0.0048, -0.0069,\n",
      "         0.0098,  0.0023,  0.0064,  0.0074, -0.0019, -0.0042, -0.0019,  0.0096,\n",
      "        -0.0034, -0.0035, -0.0003, -0.0013,  0.0009,  0.0089,  0.0085, -0.0061,\n",
      "         0.0065, -0.0008, -0.0024, -0.0078,  0.0097, -0.0083, -0.0049,  0.0014,\n",
      "         0.0043,  0.0070,  0.0014,  0.0009, -0.0013,  0.0006,  0.0061,  0.0041,\n",
      "         0.0046, -0.0028, -0.0021, -0.0086,  0.0087,  0.0025, -0.0031, -0.0067,\n",
      "        -0.0030,  0.0056, -0.0038,  0.0060,  0.0073, -0.0057,  0.0059,  0.0086,\n",
      "         0.0032, -0.0063,  0.0070, -0.0047, -0.0081,  0.0057,  0.0003, -0.0098,\n",
      "        -0.0031, -0.0089,  0.0073,  0.0053,  0.0027, -0.0094, -0.0054,  0.0052,\n",
      "        -0.0035, -0.0060,  0.0083, -0.0027,  0.0017, -0.0019,  0.0002,  0.0056,\n",
      "         0.0055, -0.0066, -0.0005,  0.0020, -0.0049,  0.0048, -0.0054, -0.0057,\n",
      "        -0.0057,  0.0091, -0.0077, -0.0068], requires_grad=True))\n",
      "('W2.weight', Parameter containing:\n",
      "tensor([[-0.0539, -0.0857,  0.0937,  ..., -0.0679, -0.0191,  0.0930],\n",
      "        [ 0.0799, -0.0321,  0.0100,  ...,  0.0737,  0.0161,  0.0673],\n",
      "        [ 0.0928,  0.0124, -0.0044,  ...,  0.0111,  0.0177,  0.0267],\n",
      "        ...,\n",
      "        [ 0.0175,  0.0666, -0.0022,  ...,  0.0077, -0.0413, -0.0171],\n",
      "        [ 0.0801, -0.0998, -0.0901,  ...,  0.0740, -0.0346, -0.0019],\n",
      "        [ 0.0900,  0.0881, -0.0752,  ...,  0.0259, -0.0374,  0.0719]],\n",
      "       requires_grad=True))\n",
      "('W2.bias', Parameter containing:\n",
      "tensor([ 0.0490,  0.0888, -0.0116,  ...,  0.0266, -0.0090, -0.0085],\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(word2vec.named_parameters()):\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(word2vec.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tensor(9.2366, grad_fn=<NllLossBackward>)\n",
      "19 tensor(9.2156, grad_fn=<NllLossBackward>)\n",
      "29 tensor(9.2081, grad_fn=<NllLossBackward>)\n",
      "39 tensor(9.2061, grad_fn=<NllLossBackward>)\n",
      "49 tensor(9.2480, grad_fn=<NllLossBackward>)\n",
      "59 tensor(9.2049, grad_fn=<NllLossBackward>)\n",
      "69 tensor(9.1707, grad_fn=<NllLossBackward>)\n",
      "79 tensor(9.1649, grad_fn=<NllLossBackward>)\n",
      "89 tensor(9.1863, grad_fn=<NllLossBackward>)\n",
      "99 tensor(9.2021, grad_fn=<NllLossBackward>)\n",
      "109 tensor(9.2138, grad_fn=<NllLossBackward>)\n",
      "119 tensor(9.1884, grad_fn=<NllLossBackward>)\n",
      "129 tensor(9.1785, grad_fn=<NllLossBackward>)\n",
      "139 tensor(9.1976, grad_fn=<NllLossBackward>)\n",
      "149 tensor(9.2229, grad_fn=<NllLossBackward>)\n",
      "159 tensor(9.2137, grad_fn=<NllLossBackward>)\n",
      "169 tensor(9.2041, grad_fn=<NllLossBackward>)\n",
      "179 tensor(9.2244, grad_fn=<NllLossBackward>)\n",
      "189 tensor(9.2095, grad_fn=<NllLossBackward>)\n",
      "199 tensor(9.2138, grad_fn=<NllLossBackward>)\n",
      "209 tensor(9.2312, grad_fn=<NllLossBackward>)\n",
      "219 tensor(9.2466, grad_fn=<NllLossBackward>)\n",
      "229 tensor(9.2023, grad_fn=<NllLossBackward>)\n",
      "239 tensor(9.2341, grad_fn=<NllLossBackward>)\n",
      "249 tensor(9.2448, grad_fn=<NllLossBackward>)\n",
      "259 tensor(9.2057, grad_fn=<NllLossBackward>)\n",
      "269 tensor(9.2419, grad_fn=<NllLossBackward>)\n",
      "279 tensor(9.2041, grad_fn=<NllLossBackward>)\n",
      "289 tensor(9.1960, grad_fn=<NllLossBackward>)\n",
      "299 tensor(9.2161, grad_fn=<NllLossBackward>)\n",
      "309 tensor(9.2081, grad_fn=<NllLossBackward>)\n",
      "319 tensor(9.1733, grad_fn=<NllLossBackward>)\n",
      "329 tensor(9.1698, grad_fn=<NllLossBackward>)\n",
      "339 tensor(9.2122, grad_fn=<NllLossBackward>)\n",
      "349 tensor(9.2214, grad_fn=<NllLossBackward>)\n",
      "359 tensor(9.1784, grad_fn=<NllLossBackward>)\n",
      "369 tensor(9.2279, grad_fn=<NllLossBackward>)\n",
      "379 tensor(9.2026, grad_fn=<NllLossBackward>)\n",
      "389 tensor(9.2231, grad_fn=<NllLossBackward>)\n",
      "399 tensor(9.2224, grad_fn=<NllLossBackward>)\n",
      "409 tensor(9.1914, grad_fn=<NllLossBackward>)\n",
      "419 tensor(9.2011, grad_fn=<NllLossBackward>)\n",
      "429 tensor(9.2538, grad_fn=<NllLossBackward>)\n",
      "439 tensor(9.2149, grad_fn=<NllLossBackward>)\n",
      "449 tensor(9.2401, grad_fn=<NllLossBackward>)\n",
      "459 tensor(9.2462, grad_fn=<NllLossBackward>)\n",
      "469 tensor(9.1874, grad_fn=<NllLossBackward>)\n",
      "479 tensor(9.2199, grad_fn=<NllLossBackward>)\n",
      "489 tensor(9.1976, grad_fn=<NllLossBackward>)\n",
      "499 tensor(9.2041, grad_fn=<NllLossBackward>)\n",
      "509 tensor(9.1932, grad_fn=<NllLossBackward>)\n",
      "519 tensor(9.2423, grad_fn=<NllLossBackward>)\n",
      "529 tensor(9.1992, grad_fn=<NllLossBackward>)\n",
      "539 tensor(9.2551, grad_fn=<NllLossBackward>)\n",
      "549 tensor(9.1950, grad_fn=<NllLossBackward>)\n",
      "559 tensor(9.2039, grad_fn=<NllLossBackward>)\n",
      "569 tensor(9.2313, grad_fn=<NllLossBackward>)\n",
      "579 tensor(9.2471, grad_fn=<NllLossBackward>)\n",
      "589 tensor(9.2270, grad_fn=<NllLossBackward>)\n",
      "599 tensor(9.1870, grad_fn=<NllLossBackward>)\n",
      "609 tensor(9.2236, grad_fn=<NllLossBackward>)\n",
      "619 tensor(9.2115, grad_fn=<NllLossBackward>)\n",
      "629 tensor(9.1776, grad_fn=<NllLossBackward>)\n",
      "639 tensor(9.2319, grad_fn=<NllLossBackward>)\n",
      "649 tensor(9.1964, grad_fn=<NllLossBackward>)\n",
      "659 tensor(9.2132, grad_fn=<NllLossBackward>)\n",
      "669 tensor(9.2034, grad_fn=<NllLossBackward>)\n",
      "679 tensor(9.1846, grad_fn=<NllLossBackward>)\n",
      "689 tensor(9.2105, grad_fn=<NllLossBackward>)\n",
      "699 tensor(9.2038, grad_fn=<NllLossBackward>)\n",
      "709 tensor(9.2235, grad_fn=<NllLossBackward>)\n",
      "719 tensor(9.1862, grad_fn=<NllLossBackward>)\n",
      "729 tensor(9.1862, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-5b5d2c67ea08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_gram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-141-485f026bca01>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(n_gram)):\n",
    "    y_pred = word2vec(n_gram[i][0])\n",
    "    y = torch.tensor([word_to_id[w] for w in n_gram[i][1]], dtype=torch.long)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    loss = loss_f(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 10 == 9 :\n",
    "        print(i, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec_cbow(nn.Module):\n",
    "    \"\"\"\n",
    "        데이터를 받으면 \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,data=n_gram, vocab=vocab, window_size=WINDOW_SIZE,\n",
    "                 word_to_id=word_to_id, id_to_word=id_to_word, hidden_unit=HIDDEN_UNIT):\n",
    "        super(Word2vec_cbow, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.size = len(vocab)\n",
    "        self.hidden = hidden_unit\n",
    "        self.W1 = torch.nn.Linear(len(self.word_to_id),self.hidden)\n",
    "        self.W2 = torch.nn.Linear(self.hidden,len(self.word_to_id))\n",
    "\n",
    "\n",
    "        self.window_size = WINDOW_SIZE\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = torch.zeros(2*self.window_size, self.size)\n",
    "        for i in range(2*self.window_size):\n",
    "            z[i, x[i]] += 1\n",
    "        h = self.W1(z).sum(dim=0)\n",
    "        return self.softmax(self.W2(h))\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2vec_cbow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 4, 5, 6]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [word_to_id[w] for w in n_gram[0][1]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-9.2589, -9.2380, -9.1977,  ..., -9.1968, -9.3090, -9.2293],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cbow(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([word_to_id[n_gram[0][0]]], dtype=torch.long)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2076, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f(y_pred.view(1,-1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([342])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(word_to_id[n_gram[1100][0]], dtype=torch.long).view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(cbow.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tensor(9.2041, grad_fn=<DivBackward0>)\n",
      "2000 tensor(9.1792, grad_fn=<DivBackward0>)\n",
      "3000 tensor(9.1729, grad_fn=<DivBackward0>)\n",
      "4000 tensor(9.1554, grad_fn=<DivBackward0>)\n",
      "5000 tensor(9.1491, grad_fn=<DivBackward0>)\n",
      "6000 tensor(9.1150, grad_fn=<DivBackward0>)\n",
      "7000 tensor(9.1077, grad_fn=<DivBackward0>)\n",
      "8000 tensor(9.0939, grad_fn=<DivBackward0>)\n",
      "9000 tensor(9.0479, grad_fn=<DivBackward0>)\n",
      "10000 tensor(9.0121, grad_fn=<DivBackward0>)\n",
      "11000 tensor(9.0091, grad_fn=<DivBackward0>)\n",
      "12000 tensor(8.9370, grad_fn=<DivBackward0>)\n",
      "13000 tensor(8.9541, grad_fn=<DivBackward0>)\n",
      "14000 tensor(8.9186, grad_fn=<DivBackward0>)\n",
      "15000 tensor(8.8790, grad_fn=<DivBackward0>)\n",
      "16000 tensor(8.8631, grad_fn=<DivBackward0>)\n",
      "17000 tensor(8.8526, grad_fn=<DivBackward0>)\n",
      "18000 tensor(8.7179, grad_fn=<DivBackward0>)\n",
      "19000 tensor(8.6109, grad_fn=<DivBackward0>)\n",
      "20000 tensor(8.5900, grad_fn=<DivBackward0>)\n",
      "21000 tensor(8.6058, grad_fn=<DivBackward0>)\n",
      "22000 tensor(8.4163, grad_fn=<DivBackward0>)\n",
      "23000 tensor(8.3715, grad_fn=<DivBackward0>)\n",
      "24000 tensor(8.3467, grad_fn=<DivBackward0>)\n",
      "25000 tensor(8.1732, grad_fn=<DivBackward0>)\n",
      "26000 tensor(8.1924, grad_fn=<DivBackward0>)\n",
      "27000 tensor(7.9649, grad_fn=<DivBackward0>)\n",
      "28000 tensor(7.4660, grad_fn=<DivBackward0>)\n",
      "29000 tensor(7.9430, grad_fn=<DivBackward0>)\n",
      "30000 tensor(7.7697, grad_fn=<DivBackward0>)\n",
      "31000 tensor(8.0163, grad_fn=<DivBackward0>)\n",
      "32000 tensor(7.8398, grad_fn=<DivBackward0>)\n",
      "33000 tensor(7.7927, grad_fn=<DivBackward0>)\n",
      "34000 tensor(7.9066, grad_fn=<DivBackward0>)\n",
      "35000 tensor(7.6501, grad_fn=<DivBackward0>)\n",
      "36000 tensor(7.4086, grad_fn=<DivBackward0>)\n",
      "37000 tensor(7.3741, grad_fn=<DivBackward0>)\n",
      "38000 tensor(7.5703, grad_fn=<DivBackward0>)\n",
      "39000 tensor(7.6240, grad_fn=<DivBackward0>)\n",
      "40000 tensor(7.6232, grad_fn=<DivBackward0>)\n",
      "41000 tensor(7.5229, grad_fn=<DivBackward0>)\n",
      "42000 tensor(7.6964, grad_fn=<DivBackward0>)\n",
      "43000 tensor(7.5214, grad_fn=<DivBackward0>)\n",
      "44000 tensor(7.6006, grad_fn=<DivBackward0>)\n",
      "45000 tensor(7.3432, grad_fn=<DivBackward0>)\n",
      "46000 tensor(7.3340, grad_fn=<DivBackward0>)\n",
      "47000 tensor(7.5103, grad_fn=<DivBackward0>)\n",
      "48000 tensor(7.4409, grad_fn=<DivBackward0>)\n",
      "49000 tensor(7.5562, grad_fn=<DivBackward0>)\n",
      "50000 tensor(7.0745, grad_fn=<DivBackward0>)\n",
      "51000 tensor(7.3422, grad_fn=<DivBackward0>)\n",
      "52000 tensor(7.2574, grad_fn=<DivBackward0>)\n",
      "53000 tensor(7.4082, grad_fn=<DivBackward0>)\n",
      "54000 tensor(7.6875, grad_fn=<DivBackward0>)\n",
      "55000 tensor(7.4707, grad_fn=<DivBackward0>)\n",
      "56000 tensor(7.4830, grad_fn=<DivBackward0>)\n",
      "57000 tensor(7.4687, grad_fn=<DivBackward0>)\n",
      "58000 tensor(7.5124, grad_fn=<DivBackward0>)\n",
      "59000 tensor(7.5977, grad_fn=<DivBackward0>)\n",
      "60000 tensor(7.3014, grad_fn=<DivBackward0>)\n",
      "61000 tensor(7.3215, grad_fn=<DivBackward0>)\n",
      "62000 tensor(7.4717, grad_fn=<DivBackward0>)\n",
      "63000 tensor(6.6487, grad_fn=<DivBackward0>)\n",
      "64000 tensor(7.4010, grad_fn=<DivBackward0>)\n",
      "65000 tensor(7.4369, grad_fn=<DivBackward0>)\n",
      "66000 tensor(7.4084, grad_fn=<DivBackward0>)\n",
      "67000 tensor(7.1742, grad_fn=<DivBackward0>)\n",
      "68000 tensor(7.2307, grad_fn=<DivBackward0>)\n",
      "69000 tensor(7.1233, grad_fn=<DivBackward0>)\n",
      "70000 tensor(6.9984, grad_fn=<DivBackward0>)\n",
      "71000 tensor(7.2262, grad_fn=<DivBackward0>)\n",
      "72000 tensor(7.4964, grad_fn=<DivBackward0>)\n",
      "73000 tensor(6.9567, grad_fn=<DivBackward0>)\n",
      "74000 tensor(7.3851, grad_fn=<DivBackward0>)\n",
      "75000 tensor(7.2376, grad_fn=<DivBackward0>)\n",
      "76000 tensor(6.9651, grad_fn=<DivBackward0>)\n",
      "77000 tensor(6.8335, grad_fn=<DivBackward0>)\n",
      "78000 tensor(6.7448, grad_fn=<DivBackward0>)\n",
      "79000 tensor(7.0070, grad_fn=<DivBackward0>)\n",
      "80000 tensor(7.2395, grad_fn=<DivBackward0>)\n",
      "81000 tensor(7.4090, grad_fn=<DivBackward0>)\n",
      "82000 tensor(7.1289, grad_fn=<DivBackward0>)\n",
      "83000 tensor(7.1851, grad_fn=<DivBackward0>)\n",
      "84000 tensor(7.0647, grad_fn=<DivBackward0>)\n",
      "85000 tensor(7.2004, grad_fn=<DivBackward0>)\n",
      "86000 tensor(7.1291, grad_fn=<DivBackward0>)\n",
      "87000 tensor(7.1443, grad_fn=<DivBackward0>)\n",
      "88000 tensor(7.1948, grad_fn=<DivBackward0>)\n",
      "89000 tensor(6.7507, grad_fn=<DivBackward0>)\n",
      "90000 tensor(6.9185, grad_fn=<DivBackward0>)\n",
      "91000 tensor(7.0682, grad_fn=<DivBackward0>)\n",
      "92000 tensor(6.8263, grad_fn=<DivBackward0>)\n",
      "93000 tensor(6.8542, grad_fn=<DivBackward0>)\n",
      "94000 tensor(6.6244, grad_fn=<DivBackward0>)\n",
      "95000 tensor(7.2111, grad_fn=<DivBackward0>)\n",
      "96000 tensor(7.0691, grad_fn=<DivBackward0>)\n",
      "97000 tensor(6.9309, grad_fn=<DivBackward0>)\n",
      "98000 tensor(6.7983, grad_fn=<DivBackward0>)\n",
      "99000 tensor(6.8016, grad_fn=<DivBackward0>)\n",
      "100000 tensor(7.2813, grad_fn=<DivBackward0>)\n",
      "101000 tensor(6.9713, grad_fn=<DivBackward0>)\n",
      "102000 tensor(7.0666, grad_fn=<DivBackward0>)\n",
      "103000 tensor(6.8558, grad_fn=<DivBackward0>)\n",
      "104000 tensor(6.7917, grad_fn=<DivBackward0>)\n",
      "105000 tensor(7.1743, grad_fn=<DivBackward0>)\n",
      "106000 tensor(6.9986, grad_fn=<DivBackward0>)\n",
      "107000 tensor(6.8118, grad_fn=<DivBackward0>)\n",
      "108000 tensor(7.0973, grad_fn=<DivBackward0>)\n",
      "109000 tensor(7.1342, grad_fn=<DivBackward0>)\n",
      "110000 tensor(6.8899, grad_fn=<DivBackward0>)\n",
      "111000 tensor(6.7133, grad_fn=<DivBackward0>)\n",
      "112000 tensor(7.2772, grad_fn=<DivBackward0>)\n",
      "113000 tensor(7.2148, grad_fn=<DivBackward0>)\n",
      "114000 tensor(6.9173, grad_fn=<DivBackward0>)\n",
      "115000 tensor(7.2405, grad_fn=<DivBackward0>)\n",
      "116000 tensor(7.0815, grad_fn=<DivBackward0>)\n",
      "117000 tensor(6.9069, grad_fn=<DivBackward0>)\n",
      "118000 tensor(6.8439, grad_fn=<DivBackward0>)\n",
      "119000 tensor(6.8298, grad_fn=<DivBackward0>)\n",
      "120000 tensor(7.0119, grad_fn=<DivBackward0>)\n",
      "121000 tensor(6.9528, grad_fn=<DivBackward0>)\n",
      "122000 tensor(6.9309, grad_fn=<DivBackward0>)\n",
      "123000 tensor(7.2809, grad_fn=<DivBackward0>)\n",
      "124000 tensor(7.1652, grad_fn=<DivBackward0>)\n",
      "125000 tensor(6.9051, grad_fn=<DivBackward0>)\n",
      "126000 tensor(6.9302, grad_fn=<DivBackward0>)\n",
      "127000 tensor(6.9975, grad_fn=<DivBackward0>)\n",
      "128000 tensor(7.2622, grad_fn=<DivBackward0>)\n",
      "129000 tensor(6.9452, grad_fn=<DivBackward0>)\n",
      "130000 tensor(6.9636, grad_fn=<DivBackward0>)\n",
      "131000 tensor(6.9615, grad_fn=<DivBackward0>)\n",
      "132000 tensor(6.9077, grad_fn=<DivBackward0>)\n",
      "133000 tensor(7.0899, grad_fn=<DivBackward0>)\n",
      "134000 tensor(7.0068, grad_fn=<DivBackward0>)\n",
      "135000 tensor(6.9080, grad_fn=<DivBackward0>)\n",
      "136000 tensor(6.8061, grad_fn=<DivBackward0>)\n",
      "137000 tensor(7.0237, grad_fn=<DivBackward0>)\n",
      "138000 tensor(6.9906, grad_fn=<DivBackward0>)\n",
      "139000 tensor(6.8792, grad_fn=<DivBackward0>)\n",
      "140000 tensor(7.0145, grad_fn=<DivBackward0>)\n",
      "141000 tensor(6.7107, grad_fn=<DivBackward0>)\n",
      "142000 tensor(6.8948, grad_fn=<DivBackward0>)\n",
      "143000 tensor(6.9406, grad_fn=<DivBackward0>)\n",
      "144000 tensor(6.6985, grad_fn=<DivBackward0>)\n",
      "145000 tensor(6.9992, grad_fn=<DivBackward0>)\n",
      "146000 tensor(6.9255, grad_fn=<DivBackward0>)\n",
      "147000 tensor(6.9601, grad_fn=<DivBackward0>)\n",
      "148000 tensor(7.0491, grad_fn=<DivBackward0>)\n",
      "149000 tensor(6.9931, grad_fn=<DivBackward0>)\n",
      "150000 tensor(7.0028, grad_fn=<DivBackward0>)\n",
      "151000 tensor(6.5563, grad_fn=<DivBackward0>)\n",
      "152000 tensor(6.8405, grad_fn=<DivBackward0>)\n",
      "153000 tensor(6.8387, grad_fn=<DivBackward0>)\n",
      "154000 tensor(7.0122, grad_fn=<DivBackward0>)\n",
      "155000 tensor(6.8483, grad_fn=<DivBackward0>)\n",
      "156000 tensor(6.7833, grad_fn=<DivBackward0>)\n",
      "157000 tensor(6.8668, grad_fn=<DivBackward0>)\n",
      "158000 tensor(7.1253, grad_fn=<DivBackward0>)\n",
      "159000 tensor(6.9947, grad_fn=<DivBackward0>)\n",
      "160000 tensor(7.0332, grad_fn=<DivBackward0>)\n",
      "161000 tensor(7.0285, grad_fn=<DivBackward0>)\n",
      "162000 tensor(6.7813, grad_fn=<DivBackward0>)\n",
      "163000 tensor(6.9636, grad_fn=<DivBackward0>)\n",
      "164000 tensor(6.9938, grad_fn=<DivBackward0>)\n",
      "165000 tensor(6.9285, grad_fn=<DivBackward0>)\n",
      "166000 tensor(6.6362, grad_fn=<DivBackward0>)\n",
      "167000 tensor(6.7081, grad_fn=<DivBackward0>)\n",
      "168000 tensor(6.9456, grad_fn=<DivBackward0>)\n",
      "169000 tensor(6.9036, grad_fn=<DivBackward0>)\n",
      "170000 tensor(6.7888, grad_fn=<DivBackward0>)\n",
      "171000 tensor(6.8752, grad_fn=<DivBackward0>)\n",
      "172000 tensor(6.5446, grad_fn=<DivBackward0>)\n",
      "173000 tensor(6.8354, grad_fn=<DivBackward0>)\n",
      "174000 tensor(6.5836, grad_fn=<DivBackward0>)\n",
      "175000 tensor(6.8654, grad_fn=<DivBackward0>)\n",
      "176000 tensor(6.8650, grad_fn=<DivBackward0>)\n",
      "177000 tensor(7.0092, grad_fn=<DivBackward0>)\n",
      "178000 tensor(6.9693, grad_fn=<DivBackward0>)\n",
      "179000 tensor(7.1256, grad_fn=<DivBackward0>)\n",
      "180000 tensor(6.9705, grad_fn=<DivBackward0>)\n",
      "181000 tensor(6.8913, grad_fn=<DivBackward0>)\n",
      "182000 tensor(6.8944, grad_fn=<DivBackward0>)\n",
      "183000 tensor(7.0537, grad_fn=<DivBackward0>)\n",
      "184000 tensor(6.7366, grad_fn=<DivBackward0>)\n",
      "185000 tensor(7.1169, grad_fn=<DivBackward0>)\n",
      "186000 tensor(7.0225, grad_fn=<DivBackward0>)\n",
      "187000 tensor(6.8299, grad_fn=<DivBackward0>)\n",
      "188000 tensor(7.0573, grad_fn=<DivBackward0>)\n",
      "189000 tensor(6.7382, grad_fn=<DivBackward0>)\n",
      "190000 tensor(6.8741, grad_fn=<DivBackward0>)\n",
      "191000 tensor(6.9439, grad_fn=<DivBackward0>)\n",
      "192000 tensor(7.1948, grad_fn=<DivBackward0>)\n",
      "193000 tensor(6.5776, grad_fn=<DivBackward0>)\n",
      "194000 tensor(6.6631, grad_fn=<DivBackward0>)\n",
      "195000 tensor(7.0621, grad_fn=<DivBackward0>)\n",
      "196000 tensor(6.9347, grad_fn=<DivBackward0>)\n",
      "197000 tensor(6.6558, grad_fn=<DivBackward0>)\n",
      "198000 tensor(6.7522, grad_fn=<DivBackward0>)\n",
      "199000 tensor(6.8217, grad_fn=<DivBackward0>)\n",
      "200000 tensor(6.7733, grad_fn=<DivBackward0>)\n",
      "201000 tensor(6.8910, grad_fn=<DivBackward0>)\n",
      "202000 tensor(6.9437, grad_fn=<DivBackward0>)\n",
      "203000 tensor(7.0375, grad_fn=<DivBackward0>)\n",
      "204000 tensor(6.8052, grad_fn=<DivBackward0>)\n",
      "205000 tensor(7.1301, grad_fn=<DivBackward0>)\n",
      "206000 tensor(6.7236, grad_fn=<DivBackward0>)\n",
      "207000 tensor(7.0813, grad_fn=<DivBackward0>)\n",
      "208000 tensor(6.9305, grad_fn=<DivBackward0>)\n",
      "209000 tensor(6.8145, grad_fn=<DivBackward0>)\n",
      "210000 tensor(7.0123, grad_fn=<DivBackward0>)\n",
      "211000 tensor(6.9424, grad_fn=<DivBackward0>)\n",
      "212000 tensor(6.9721, grad_fn=<DivBackward0>)\n",
      "213000 tensor(6.8084, grad_fn=<DivBackward0>)\n",
      "214000 tensor(6.9167, grad_fn=<DivBackward0>)\n",
      "215000 tensor(6.6078, grad_fn=<DivBackward0>)\n",
      "216000 tensor(6.7793, grad_fn=<DivBackward0>)\n",
      "217000 tensor(6.6788, grad_fn=<DivBackward0>)\n",
      "218000 tensor(6.9298, grad_fn=<DivBackward0>)\n",
      "219000 tensor(6.5419, grad_fn=<DivBackward0>)\n",
      "220000 tensor(6.8920, grad_fn=<DivBackward0>)\n",
      "221000 tensor(6.5359, grad_fn=<DivBackward0>)\n",
      "222000 tensor(6.9980, grad_fn=<DivBackward0>)\n",
      "223000 tensor(7.1116, grad_fn=<DivBackward0>)\n",
      "224000 tensor(6.8527, grad_fn=<DivBackward0>)\n",
      "225000 tensor(6.4011, grad_fn=<DivBackward0>)\n",
      "226000 tensor(6.8131, grad_fn=<DivBackward0>)\n",
      "227000 tensor(6.9391, grad_fn=<DivBackward0>)\n",
      "228000 tensor(6.7720, grad_fn=<DivBackward0>)\n",
      "229000 tensor(6.8016, grad_fn=<DivBackward0>)\n",
      "230000 tensor(6.7823, grad_fn=<DivBackward0>)\n",
      "231000 tensor(6.7300, grad_fn=<DivBackward0>)\n",
      "232000 tensor(6.8373, grad_fn=<DivBackward0>)\n",
      "233000 tensor(6.8676, grad_fn=<DivBackward0>)\n",
      "234000 tensor(6.7914, grad_fn=<DivBackward0>)\n",
      "235000 tensor(6.7695, grad_fn=<DivBackward0>)\n",
      "236000 tensor(6.6466, grad_fn=<DivBackward0>)\n",
      "237000 tensor(6.8632, grad_fn=<DivBackward0>)\n",
      "238000 tensor(6.7945, grad_fn=<DivBackward0>)\n",
      "239000 tensor(6.8998, grad_fn=<DivBackward0>)\n",
      "240000 tensor(6.9410, grad_fn=<DivBackward0>)\n",
      "241000 tensor(6.7287, grad_fn=<DivBackward0>)\n",
      "242000 tensor(6.9256, grad_fn=<DivBackward0>)\n",
      "243000 tensor(6.5406, grad_fn=<DivBackward0>)\n",
      "244000 tensor(6.7876, grad_fn=<DivBackward0>)\n",
      "245000 tensor(6.7543, grad_fn=<DivBackward0>)\n",
      "246000 tensor(6.7569, grad_fn=<DivBackward0>)\n",
      "247000 tensor(6.9015, grad_fn=<DivBackward0>)\n",
      "248000 tensor(6.7969, grad_fn=<DivBackward0>)\n",
      "249000 tensor(6.8871, grad_fn=<DivBackward0>)\n",
      "250000 tensor(6.8480, grad_fn=<DivBackward0>)\n",
      "251000 tensor(6.6560, grad_fn=<DivBackward0>)\n",
      "252000 tensor(6.8805, grad_fn=<DivBackward0>)\n",
      "253000 tensor(6.7483, grad_fn=<DivBackward0>)\n",
      "254000 tensor(6.6270, grad_fn=<DivBackward0>)\n",
      "255000 tensor(6.8629, grad_fn=<DivBackward0>)\n",
      "256000 tensor(6.8707, grad_fn=<DivBackward0>)\n",
      "257000 tensor(6.9322, grad_fn=<DivBackward0>)\n",
      "258000 tensor(7.0080, grad_fn=<DivBackward0>)\n",
      "259000 tensor(6.7904, grad_fn=<DivBackward0>)\n",
      "260000 tensor(7.0959, grad_fn=<DivBackward0>)\n",
      "261000 tensor(6.7416, grad_fn=<DivBackward0>)\n",
      "262000 tensor(6.6473, grad_fn=<DivBackward0>)\n",
      "263000 tensor(6.9070, grad_fn=<DivBackward0>)\n",
      "264000 tensor(6.4828, grad_fn=<DivBackward0>)\n",
      "265000 tensor(6.7049, grad_fn=<DivBackward0>)\n",
      "266000 tensor(6.7578, grad_fn=<DivBackward0>)\n",
      "267000 tensor(6.9410, grad_fn=<DivBackward0>)\n",
      "268000 tensor(6.8499, grad_fn=<DivBackward0>)\n",
      "269000 tensor(6.6362, grad_fn=<DivBackward0>)\n",
      "270000 tensor(6.8715, grad_fn=<DivBackward0>)\n",
      "271000 tensor(6.8804, grad_fn=<DivBackward0>)\n",
      "272000 tensor(6.9277, grad_fn=<DivBackward0>)\n",
      "273000 tensor(6.8060, grad_fn=<DivBackward0>)\n",
      "274000 tensor(6.8184, grad_fn=<DivBackward0>)\n",
      "275000 tensor(6.8424, grad_fn=<DivBackward0>)\n",
      "276000 tensor(6.6254, grad_fn=<DivBackward0>)\n",
      "277000 tensor(6.7864, grad_fn=<DivBackward0>)\n",
      "278000 tensor(6.7843, grad_fn=<DivBackward0>)\n",
      "279000 tensor(6.6658, grad_fn=<DivBackward0>)\n",
      "280000 tensor(6.7522, grad_fn=<DivBackward0>)\n",
      "281000 tensor(6.6153, grad_fn=<DivBackward0>)\n",
      "282000 tensor(6.5464, grad_fn=<DivBackward0>)\n",
      "283000 tensor(6.8382, grad_fn=<DivBackward0>)\n",
      "284000 tensor(6.8024, grad_fn=<DivBackward0>)\n",
      "285000 tensor(6.9308, grad_fn=<DivBackward0>)\n",
      "286000 tensor(6.3461, grad_fn=<DivBackward0>)\n",
      "287000 tensor(6.7078, grad_fn=<DivBackward0>)\n",
      "288000 tensor(6.6930, grad_fn=<DivBackward0>)\n",
      "289000 tensor(6.6875, grad_fn=<DivBackward0>)\n",
      "290000 tensor(6.6595, grad_fn=<DivBackward0>)\n",
      "291000 tensor(6.7967, grad_fn=<DivBackward0>)\n",
      "292000 tensor(6.5992, grad_fn=<DivBackward0>)\n",
      "293000 tensor(6.8420, grad_fn=<DivBackward0>)\n",
      "294000 tensor(6.9195, grad_fn=<DivBackward0>)\n",
      "295000 tensor(6.7381, grad_fn=<DivBackward0>)\n",
      "296000 tensor(6.3107, grad_fn=<DivBackward0>)\n",
      "297000 tensor(6.7612, grad_fn=<DivBackward0>)\n",
      "298000 tensor(6.8896, grad_fn=<DivBackward0>)\n",
      "299000 tensor(6.7449, grad_fn=<DivBackward0>)\n",
      "300000 tensor(6.8928, grad_fn=<DivBackward0>)\n",
      "301000 tensor(7.0280, grad_fn=<DivBackward0>)\n",
      "302000 tensor(6.6150, grad_fn=<DivBackward0>)\n",
      "303000 tensor(6.7513, grad_fn=<DivBackward0>)\n",
      "304000 tensor(6.7056, grad_fn=<DivBackward0>)\n",
      "305000 tensor(6.5866, grad_fn=<DivBackward0>)\n",
      "306000 tensor(6.4334, grad_fn=<DivBackward0>)\n",
      "307000 tensor(6.4043, grad_fn=<DivBackward0>)\n",
      "308000 tensor(6.4340, grad_fn=<DivBackward0>)\n",
      "309000 tensor(6.7314, grad_fn=<DivBackward0>)\n",
      "310000 tensor(6.7239, grad_fn=<DivBackward0>)\n",
      "311000 tensor(6.8437, grad_fn=<DivBackward0>)\n",
      "312000 tensor(6.6890, grad_fn=<DivBackward0>)\n",
      "313000 tensor(6.5360, grad_fn=<DivBackward0>)\n",
      "314000 tensor(6.9303, grad_fn=<DivBackward0>)\n",
      "315000 tensor(7.0041, grad_fn=<DivBackward0>)\n",
      "316000 tensor(6.6912, grad_fn=<DivBackward0>)\n",
      "317000 tensor(6.6839, grad_fn=<DivBackward0>)\n",
      "318000 tensor(6.7917, grad_fn=<DivBackward0>)\n",
      "319000 tensor(6.8551, grad_fn=<DivBackward0>)\n",
      "320000 tensor(6.8070, grad_fn=<DivBackward0>)\n",
      "321000 tensor(6.8329, grad_fn=<DivBackward0>)\n",
      "322000 tensor(6.9218, grad_fn=<DivBackward0>)\n",
      "323000 tensor(6.6059, grad_fn=<DivBackward0>)\n",
      "324000 tensor(6.7943, grad_fn=<DivBackward0>)\n",
      "325000 tensor(6.8172, grad_fn=<DivBackward0>)\n",
      "326000 tensor(6.6464, grad_fn=<DivBackward0>)\n",
      "327000 tensor(6.4975, grad_fn=<DivBackward0>)\n",
      "328000 tensor(6.5080, grad_fn=<DivBackward0>)\n",
      "329000 tensor(6.5788, grad_fn=<DivBackward0>)\n",
      "330000 tensor(6.7857, grad_fn=<DivBackward0>)\n",
      "331000 tensor(6.7684, grad_fn=<DivBackward0>)\n",
      "332000 tensor(6.8551, grad_fn=<DivBackward0>)\n",
      "333000 tensor(6.8208, grad_fn=<DivBackward0>)\n",
      "334000 tensor(6.7850, grad_fn=<DivBackward0>)\n",
      "335000 tensor(6.7792, grad_fn=<DivBackward0>)\n",
      "336000 tensor(6.8674, grad_fn=<DivBackward0>)\n",
      "337000 tensor(6.7711, grad_fn=<DivBackward0>)\n",
      "338000 tensor(6.8879, grad_fn=<DivBackward0>)\n",
      "339000 tensor(6.7264, grad_fn=<DivBackward0>)\n",
      "340000 tensor(6.5607, grad_fn=<DivBackward0>)\n",
      "341000 tensor(6.9682, grad_fn=<DivBackward0>)\n",
      "342000 tensor(6.6225, grad_fn=<DivBackward0>)\n",
      "343000 tensor(6.6841, grad_fn=<DivBackward0>)\n",
      "344000 tensor(6.6792, grad_fn=<DivBackward0>)\n",
      "345000 tensor(6.8058, grad_fn=<DivBackward0>)\n",
      "346000 tensor(6.7700, grad_fn=<DivBackward0>)\n",
      "347000 tensor(6.6300, grad_fn=<DivBackward0>)\n",
      "348000 tensor(6.0874, grad_fn=<DivBackward0>)\n",
      "349000 tensor(6.6795, grad_fn=<DivBackward0>)\n",
      "350000 tensor(6.6058, grad_fn=<DivBackward0>)\n",
      "351000 tensor(6.5648, grad_fn=<DivBackward0>)\n",
      "352000 tensor(6.5363, grad_fn=<DivBackward0>)\n",
      "353000 tensor(6.6183, grad_fn=<DivBackward0>)\n",
      "354000 tensor(6.5501, grad_fn=<DivBackward0>)\n",
      "355000 tensor(6.5960, grad_fn=<DivBackward0>)\n",
      "356000 tensor(6.7180, grad_fn=<DivBackward0>)\n",
      "357000 tensor(6.9228, grad_fn=<DivBackward0>)\n",
      "358000 tensor(6.4989, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359000 tensor(6.5786, grad_fn=<DivBackward0>)\n",
      "360000 tensor(6.5413, grad_fn=<DivBackward0>)\n",
      "361000 tensor(6.3726, grad_fn=<DivBackward0>)\n",
      "362000 tensor(6.4592, grad_fn=<DivBackward0>)\n",
      "363000 tensor(6.4728, grad_fn=<DivBackward0>)\n",
      "364000 tensor(6.5916, grad_fn=<DivBackward0>)\n",
      "365000 tensor(6.6369, grad_fn=<DivBackward0>)\n",
      "366000 tensor(6.6985, grad_fn=<DivBackward0>)\n",
      "367000 tensor(6.4623, grad_fn=<DivBackward0>)\n",
      "368000 tensor(6.5804, grad_fn=<DivBackward0>)\n",
      "369000 tensor(6.6657, grad_fn=<DivBackward0>)\n",
      "370000 tensor(6.4315, grad_fn=<DivBackward0>)\n",
      "371000 tensor(6.7554, grad_fn=<DivBackward0>)\n",
      "372000 tensor(6.8661, grad_fn=<DivBackward0>)\n",
      "373000 tensor(6.8029, grad_fn=<DivBackward0>)\n",
      "374000 tensor(6.8621, grad_fn=<DivBackward0>)\n",
      "375000 tensor(6.4154, grad_fn=<DivBackward0>)\n",
      "376000 tensor(6.6608, grad_fn=<DivBackward0>)\n",
      "377000 tensor(6.7679, grad_fn=<DivBackward0>)\n",
      "378000 tensor(6.8605, grad_fn=<DivBackward0>)\n",
      "379000 tensor(6.4828, grad_fn=<DivBackward0>)\n",
      "380000 tensor(6.5901, grad_fn=<DivBackward0>)\n",
      "381000 tensor(6.3975, grad_fn=<DivBackward0>)\n",
      "382000 tensor(6.8017, grad_fn=<DivBackward0>)\n",
      "383000 tensor(6.6938, grad_fn=<DivBackward0>)\n",
      "384000 tensor(6.4340, grad_fn=<DivBackward0>)\n",
      "385000 tensor(6.8888, grad_fn=<DivBackward0>)\n",
      "386000 tensor(6.8347, grad_fn=<DivBackward0>)\n",
      "387000 tensor(6.5831, grad_fn=<DivBackward0>)\n",
      "388000 tensor(6.8233, grad_fn=<DivBackward0>)\n",
      "389000 tensor(6.6083, grad_fn=<DivBackward0>)\n",
      "390000 tensor(6.5674, grad_fn=<DivBackward0>)\n",
      "391000 tensor(6.5820, grad_fn=<DivBackward0>)\n",
      "392000 tensor(6.6911, grad_fn=<DivBackward0>)\n",
      "393000 tensor(6.7706, grad_fn=<DivBackward0>)\n",
      "394000 tensor(6.8141, grad_fn=<DivBackward0>)\n",
      "395000 tensor(6.6577, grad_fn=<DivBackward0>)\n",
      "396000 tensor(6.7284, grad_fn=<DivBackward0>)\n",
      "397000 tensor(6.5710, grad_fn=<DivBackward0>)\n",
      "398000 tensor(6.6006, grad_fn=<DivBackward0>)\n",
      "399000 tensor(6.2380, grad_fn=<DivBackward0>)\n",
      "400000 tensor(6.7704, grad_fn=<DivBackward0>)\n",
      "401000 tensor(6.9240, grad_fn=<DivBackward0>)\n",
      "402000 tensor(6.6375, grad_fn=<DivBackward0>)\n",
      "403000 tensor(6.3798, grad_fn=<DivBackward0>)\n",
      "404000 tensor(6.7469, grad_fn=<DivBackward0>)\n",
      "405000 tensor(6.6941, grad_fn=<DivBackward0>)\n",
      "406000 tensor(6.6987, grad_fn=<DivBackward0>)\n",
      "407000 tensor(6.8669, grad_fn=<DivBackward0>)\n",
      "408000 tensor(6.7307, grad_fn=<DivBackward0>)\n",
      "409000 tensor(6.7388, grad_fn=<DivBackward0>)\n",
      "410000 tensor(6.0302, grad_fn=<DivBackward0>)\n",
      "411000 tensor(6.8927, grad_fn=<DivBackward0>)\n",
      "412000 tensor(6.6679, grad_fn=<DivBackward0>)\n",
      "413000 tensor(6.7651, grad_fn=<DivBackward0>)\n",
      "414000 tensor(6.7830, grad_fn=<DivBackward0>)\n",
      "415000 tensor(6.8205, grad_fn=<DivBackward0>)\n",
      "416000 tensor(6.3189, grad_fn=<DivBackward0>)\n",
      "417000 tensor(6.6138, grad_fn=<DivBackward0>)\n",
      "418000 tensor(6.6463, grad_fn=<DivBackward0>)\n",
      "419000 tensor(6.7496, grad_fn=<DivBackward0>)\n",
      "420000 tensor(6.7453, grad_fn=<DivBackward0>)\n",
      "421000 tensor(6.5321, grad_fn=<DivBackward0>)\n",
      "422000 tensor(6.8234, grad_fn=<DivBackward0>)\n",
      "423000 tensor(6.8884, grad_fn=<DivBackward0>)\n",
      "424000 tensor(6.7652, grad_fn=<DivBackward0>)\n",
      "425000 tensor(6.4942, grad_fn=<DivBackward0>)\n",
      "426000 tensor(6.4043, grad_fn=<DivBackward0>)\n",
      "427000 tensor(6.3306, grad_fn=<DivBackward0>)\n",
      "428000 tensor(6.6122, grad_fn=<DivBackward0>)\n",
      "429000 tensor(6.6810, grad_fn=<DivBackward0>)\n",
      "430000 tensor(6.7536, grad_fn=<DivBackward0>)\n",
      "431000 tensor(6.8022, grad_fn=<DivBackward0>)\n",
      "432000 tensor(6.8990, grad_fn=<DivBackward0>)\n",
      "433000 tensor(6.7331, grad_fn=<DivBackward0>)\n",
      "434000 tensor(6.5008, grad_fn=<DivBackward0>)\n",
      "435000 tensor(6.7079, grad_fn=<DivBackward0>)\n",
      "436000 tensor(6.3126, grad_fn=<DivBackward0>)\n",
      "437000 tensor(6.7299, grad_fn=<DivBackward0>)\n",
      "438000 tensor(6.7666, grad_fn=<DivBackward0>)\n",
      "439000 tensor(6.7009, grad_fn=<DivBackward0>)\n",
      "440000 tensor(6.7976, grad_fn=<DivBackward0>)\n",
      "441000 tensor(6.7541, grad_fn=<DivBackward0>)\n",
      "442000 tensor(6.7752, grad_fn=<DivBackward0>)\n",
      "443000 tensor(6.6923, grad_fn=<DivBackward0>)\n",
      "444000 tensor(6.3265, grad_fn=<DivBackward0>)\n",
      "445000 tensor(6.4450, grad_fn=<DivBackward0>)\n",
      "446000 tensor(6.5091, grad_fn=<DivBackward0>)\n",
      "447000 tensor(6.7608, grad_fn=<DivBackward0>)\n",
      "448000 tensor(6.9086, grad_fn=<DivBackward0>)\n",
      "449000 tensor(6.5253, grad_fn=<DivBackward0>)\n",
      "450000 tensor(6.7331, grad_fn=<DivBackward0>)\n",
      "451000 tensor(6.8666, grad_fn=<DivBackward0>)\n",
      "452000 tensor(6.7591, grad_fn=<DivBackward0>)\n",
      "453000 tensor(6.6363, grad_fn=<DivBackward0>)\n",
      "454000 tensor(6.5633, grad_fn=<DivBackward0>)\n",
      "455000 tensor(6.3852, grad_fn=<DivBackward0>)\n",
      "456000 tensor(6.7022, grad_fn=<DivBackward0>)\n",
      "457000 tensor(6.0697, grad_fn=<DivBackward0>)\n",
      "458000 tensor(6.6792, grad_fn=<DivBackward0>)\n",
      "459000 tensor(6.5753, grad_fn=<DivBackward0>)\n",
      "460000 tensor(6.6360, grad_fn=<DivBackward0>)\n",
      "461000 tensor(6.7343, grad_fn=<DivBackward0>)\n",
      "462000 tensor(6.7226, grad_fn=<DivBackward0>)\n",
      "463000 tensor(6.3847, grad_fn=<DivBackward0>)\n",
      "464000 tensor(6.6324, grad_fn=<DivBackward0>)\n",
      "465000 tensor(7.0428, grad_fn=<DivBackward0>)\n",
      "466000 tensor(6.8861, grad_fn=<DivBackward0>)\n",
      "467000 tensor(6.6276, grad_fn=<DivBackward0>)\n",
      "468000 tensor(6.7990, grad_fn=<DivBackward0>)\n",
      "469000 tensor(6.5088, grad_fn=<DivBackward0>)\n",
      "470000 tensor(6.5449, grad_fn=<DivBackward0>)\n",
      "471000 tensor(6.4703, grad_fn=<DivBackward0>)\n",
      "472000 tensor(6.6445, grad_fn=<DivBackward0>)\n",
      "473000 tensor(6.4718, grad_fn=<DivBackward0>)\n",
      "474000 tensor(6.4949, grad_fn=<DivBackward0>)\n",
      "475000 tensor(6.6213, grad_fn=<DivBackward0>)\n",
      "476000 tensor(6.7147, grad_fn=<DivBackward0>)\n",
      "477000 tensor(6.7588, grad_fn=<DivBackward0>)\n",
      "478000 tensor(6.3894, grad_fn=<DivBackward0>)\n",
      "479000 tensor(6.3018, grad_fn=<DivBackward0>)\n",
      "480000 tensor(6.4962, grad_fn=<DivBackward0>)\n",
      "481000 tensor(6.4546, grad_fn=<DivBackward0>)\n",
      "482000 tensor(6.3439, grad_fn=<DivBackward0>)\n",
      "483000 tensor(6.5142, grad_fn=<DivBackward0>)\n",
      "484000 tensor(6.5621, grad_fn=<DivBackward0>)\n",
      "485000 tensor(5.9123, grad_fn=<DivBackward0>)\n",
      "486000 tensor(6.6883, grad_fn=<DivBackward0>)\n",
      "487000 tensor(6.4018, grad_fn=<DivBackward0>)\n",
      "488000 tensor(6.3759, grad_fn=<DivBackward0>)\n",
      "489000 tensor(6.3860, grad_fn=<DivBackward0>)\n",
      "490000 tensor(6.4266, grad_fn=<DivBackward0>)\n",
      "491000 tensor(6.6368, grad_fn=<DivBackward0>)\n",
      "492000 tensor(6.3927, grad_fn=<DivBackward0>)\n",
      "493000 tensor(6.5331, grad_fn=<DivBackward0>)\n",
      "494000 tensor(6.5978, grad_fn=<DivBackward0>)\n",
      "495000 tensor(6.6148, grad_fn=<DivBackward0>)\n",
      "496000 tensor(6.6672, grad_fn=<DivBackward0>)\n",
      "497000 tensor(6.7406, grad_fn=<DivBackward0>)\n",
      "498000 tensor(6.5821, grad_fn=<DivBackward0>)\n",
      "499000 tensor(6.5064, grad_fn=<DivBackward0>)\n",
      "500000 tensor(6.3940, grad_fn=<DivBackward0>)\n",
      "501000 tensor(6.5987, grad_fn=<DivBackward0>)\n",
      "502000 tensor(6.5776, grad_fn=<DivBackward0>)\n",
      "503000 tensor(6.6506, grad_fn=<DivBackward0>)\n",
      "504000 tensor(6.6296, grad_fn=<DivBackward0>)\n",
      "505000 tensor(6.6471, grad_fn=<DivBackward0>)\n",
      "506000 tensor(6.7257, grad_fn=<DivBackward0>)\n",
      "507000 tensor(5.8332, grad_fn=<DivBackward0>)\n",
      "508000 tensor(6.4264, grad_fn=<DivBackward0>)\n",
      "509000 tensor(6.3024, grad_fn=<DivBackward0>)\n",
      "510000 tensor(6.3330, grad_fn=<DivBackward0>)\n",
      "511000 tensor(6.7401, grad_fn=<DivBackward0>)\n",
      "512000 tensor(6.6447, grad_fn=<DivBackward0>)\n",
      "513000 tensor(6.4988, grad_fn=<DivBackward0>)\n",
      "514000 tensor(6.6590, grad_fn=<DivBackward0>)\n",
      "515000 tensor(5.9993, grad_fn=<DivBackward0>)\n",
      "516000 tensor(6.5402, grad_fn=<DivBackward0>)\n",
      "517000 tensor(6.3086, grad_fn=<DivBackward0>)\n",
      "518000 tensor(6.7669, grad_fn=<DivBackward0>)\n",
      "519000 tensor(6.8670, grad_fn=<DivBackward0>)\n",
      "520000 tensor(6.8525, grad_fn=<DivBackward0>)\n",
      "521000 tensor(6.4244, grad_fn=<DivBackward0>)\n",
      "522000 tensor(6.7488, grad_fn=<DivBackward0>)\n",
      "523000 tensor(6.5606, grad_fn=<DivBackward0>)\n",
      "524000 tensor(6.8859, grad_fn=<DivBackward0>)\n",
      "525000 tensor(6.6919, grad_fn=<DivBackward0>)\n",
      "526000 tensor(6.7383, grad_fn=<DivBackward0>)\n",
      "527000 tensor(6.7966, grad_fn=<DivBackward0>)\n",
      "528000 tensor(6.7525, grad_fn=<DivBackward0>)\n",
      "529000 tensor(6.5398, grad_fn=<DivBackward0>)\n",
      "530000 tensor(6.8623, grad_fn=<DivBackward0>)\n",
      "531000 tensor(6.7957, grad_fn=<DivBackward0>)\n",
      "532000 tensor(6.5017, grad_fn=<DivBackward0>)\n",
      "533000 tensor(6.7482, grad_fn=<DivBackward0>)\n",
      "534000 tensor(6.7713, grad_fn=<DivBackward0>)\n",
      "535000 tensor(6.3467, grad_fn=<DivBackward0>)\n",
      "536000 tensor(6.4009, grad_fn=<DivBackward0>)\n",
      "537000 tensor(6.3893, grad_fn=<DivBackward0>)\n",
      "538000 tensor(6.7685, grad_fn=<DivBackward0>)\n",
      "539000 tensor(6.1044, grad_fn=<DivBackward0>)\n",
      "540000 tensor(6.7811, grad_fn=<DivBackward0>)\n",
      "541000 tensor(6.8106, grad_fn=<DivBackward0>)\n",
      "542000 tensor(6.4507, grad_fn=<DivBackward0>)\n",
      "543000 tensor(6.6957, grad_fn=<DivBackward0>)\n",
      "544000 tensor(6.4381, grad_fn=<DivBackward0>)\n",
      "545000 tensor(6.5418, grad_fn=<DivBackward0>)\n",
      "546000 tensor(6.5813, grad_fn=<DivBackward0>)\n",
      "547000 tensor(6.4267, grad_fn=<DivBackward0>)\n",
      "548000 tensor(6.6175, grad_fn=<DivBackward0>)\n",
      "549000 tensor(6.6151, grad_fn=<DivBackward0>)\n",
      "550000 tensor(6.4486, grad_fn=<DivBackward0>)\n",
      "551000 tensor(6.7575, grad_fn=<DivBackward0>)\n",
      "552000 tensor(6.6631, grad_fn=<DivBackward0>)\n",
      "553000 tensor(6.3002, grad_fn=<DivBackward0>)\n",
      "554000 tensor(5.7161, grad_fn=<DivBackward0>)\n",
      "555000 tensor(6.4559, grad_fn=<DivBackward0>)\n",
      "556000 tensor(6.0796, grad_fn=<DivBackward0>)\n",
      "557000 tensor(6.7024, grad_fn=<DivBackward0>)\n",
      "558000 tensor(6.7462, grad_fn=<DivBackward0>)\n",
      "559000 tensor(6.8470, grad_fn=<DivBackward0>)\n",
      "560000 tensor(6.9080, grad_fn=<DivBackward0>)\n",
      "561000 tensor(6.7283, grad_fn=<DivBackward0>)\n",
      "562000 tensor(6.4143, grad_fn=<DivBackward0>)\n",
      "563000 tensor(6.2840, grad_fn=<DivBackward0>)\n",
      "564000 tensor(6.1913, grad_fn=<DivBackward0>)\n",
      "565000 tensor(6.6236, grad_fn=<DivBackward0>)\n",
      "566000 tensor(6.4248, grad_fn=<DivBackward0>)\n",
      "567000 tensor(6.6293, grad_fn=<DivBackward0>)\n",
      "568000 tensor(6.8881, grad_fn=<DivBackward0>)\n",
      "569000 tensor(6.4552, grad_fn=<DivBackward0>)\n",
      "570000 tensor(6.3990, grad_fn=<DivBackward0>)\n",
      "571000 tensor(6.3483, grad_fn=<DivBackward0>)\n",
      "572000 tensor(6.0093, grad_fn=<DivBackward0>)\n",
      "573000 tensor(5.9886, grad_fn=<DivBackward0>)\n",
      "574000 tensor(6.4665, grad_fn=<DivBackward0>)\n",
      "575000 tensor(6.7607, grad_fn=<DivBackward0>)\n",
      "576000 tensor(6.4341, grad_fn=<DivBackward0>)\n",
      "577000 tensor(6.1324, grad_fn=<DivBackward0>)\n",
      "578000 tensor(6.6009, grad_fn=<DivBackward0>)\n",
      "579000 tensor(6.8469, grad_fn=<DivBackward0>)\n",
      "580000 tensor(6.6246, grad_fn=<DivBackward0>)\n",
      "581000 tensor(6.6136, grad_fn=<DivBackward0>)\n",
      "582000 tensor(6.7155, grad_fn=<DivBackward0>)\n",
      "583000 tensor(6.6487, grad_fn=<DivBackward0>)\n",
      "584000 tensor(6.8990, grad_fn=<DivBackward0>)\n",
      "585000 tensor(6.6122, grad_fn=<DivBackward0>)\n",
      "586000 tensor(6.7466, grad_fn=<DivBackward0>)\n",
      "587000 tensor(6.4284, grad_fn=<DivBackward0>)\n",
      "588000 tensor(6.5734, grad_fn=<DivBackward0>)\n",
      "589000 tensor(6.6330, grad_fn=<DivBackward0>)\n",
      "590000 tensor(6.5326, grad_fn=<DivBackward0>)\n",
      "591000 tensor(6.6362, grad_fn=<DivBackward0>)\n",
      "592000 tensor(6.7830, grad_fn=<DivBackward0>)\n",
      "593000 tensor(6.2863, grad_fn=<DivBackward0>)\n",
      "594000 tensor(6.7161, grad_fn=<DivBackward0>)\n",
      "595000 tensor(6.5926, grad_fn=<DivBackward0>)\n",
      "596000 tensor(6.3176, grad_fn=<DivBackward0>)\n",
      "597000 tensor(6.5744, grad_fn=<DivBackward0>)\n",
      "598000 tensor(6.6564, grad_fn=<DivBackward0>)\n",
      "599000 tensor(6.3508, grad_fn=<DivBackward0>)\n",
      "600000 tensor(6.3846, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e7dc6b14753c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\PyTorch_ev\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses=0\n",
    "results = []\n",
    "for i in range(len(n_gram)):\n",
    "    \n",
    "    z = torch.tensor([word_to_id[w] for w in n_gram[i][1]], dtype=torch.long)\n",
    "    y_pred = cbow(z).view(1,-1)\n",
    "    y = torch.tensor(word_to_id[n_gram[i][0]], dtype=torch.long).view(1)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    loss = loss_f(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    losses += loss\n",
    "    optim.step()\n",
    "    if i % 1000 == 999 :\n",
    "        print(i+1, losses/1000)\n",
    "        results.append(losses/1000)\n",
    "        losses=0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cbow.state_dict(), 'cbow.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19f663bd9b0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('W1.weight', Parameter containing:\n",
      "tensor([[-0.0058, -0.0011,  0.0053,  ..., -0.0056,  0.0037,  0.0068],\n",
      "        [-0.0063, -0.0057, -0.0035,  ...,  0.0025, -0.0087,  0.0022],\n",
      "        [-0.0100, -0.0069,  0.0035,  ...,  0.0026, -0.0042, -0.0056],\n",
      "        ...,\n",
      "        [ 0.0027, -0.0055, -0.0067,  ..., -0.0011,  0.0012,  0.0066],\n",
      "        [-0.0009, -0.0034, -0.0001,  ...,  0.0013, -0.0083,  0.0041],\n",
      "        [ 0.0016, -0.0036, -0.0064,  ...,  0.0075,  0.0026,  0.0089]],\n",
      "       requires_grad=True))\n",
      "('W1.bias', Parameter containing:\n",
      "tensor([-0.6288,  0.1632,  0.2319, -0.2753,  0.1832, -0.3666, -0.3604,  0.0961,\n",
      "         0.3225,  0.3186, -0.1721,  0.2441, -0.1717, -0.0215, -0.1328,  0.2377,\n",
      "        -0.4023, -0.1777, -0.0311, -0.2939,  0.5834,  0.2970, -0.1646, -0.1901,\n",
      "         0.0737,  0.1789,  0.3678, -0.0649, -0.0212,  0.4274,  0.2162, -0.5541,\n",
      "        -0.0227, -0.0289, -0.0528,  0.1646,  0.0302,  0.1544,  0.0020, -0.3992,\n",
      "        -0.0839, -0.0307, -0.1826,  0.4011,  0.2746,  0.1748, -0.1785,  0.3398,\n",
      "         0.0742, -0.3207,  0.0111,  0.1202,  0.0686, -0.3055, -0.3167,  0.0265,\n",
      "        -0.0283, -0.3009,  0.3465,  0.0340,  0.2675,  0.0801, -0.2403, -0.0812,\n",
      "        -0.2311,  0.0502, -0.2018, -0.2286,  0.3032,  0.1376,  0.0515, -0.3332,\n",
      "         0.2779,  0.4051, -0.2054,  0.1820, -0.0789,  0.0026,  0.1412,  0.1651,\n",
      "         0.1180, -0.3634,  0.4463,  0.1134, -0.1457,  0.2778,  0.2907,  0.1990,\n",
      "         0.3359, -0.0452,  0.2002, -0.2486, -0.0680, -0.3924,  0.1496, -0.3324,\n",
      "        -0.3852,  0.2090, -0.1975,  0.5322], requires_grad=True))\n",
      "('W2.weight', Parameter containing:\n",
      "tensor([[ 0.0499, -0.0986, -0.0886,  ...,  0.0427, -0.0440,  0.0892],\n",
      "        [-0.0489, -0.0663, -0.0877,  ..., -0.0444, -0.0378,  0.0431],\n",
      "        [-0.0785, -0.0938, -0.0299,  ...,  0.0726, -0.0146, -0.0370],\n",
      "        ...,\n",
      "        [ 0.0327,  0.0424,  0.0171,  ...,  0.0914,  0.0337,  0.0715],\n",
      "        [ 0.0078,  0.0050, -0.0644,  ...,  0.0514,  0.0027, -0.0926],\n",
      "        [-0.0887,  0.0953,  0.0297,  ..., -0.0592,  0.1025,  0.0255]],\n",
      "       requires_grad=True))\n",
      "('W2.bias', Parameter containing:\n",
      "tensor([-0.0022, -0.0995,  0.0697,  ..., -0.0281,  0.0825, -0.0111],\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(cbow.named_parameters()):\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
