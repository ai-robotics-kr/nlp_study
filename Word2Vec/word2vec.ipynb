{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 연습\n",
    "\n",
    "PTB 데이터셋을 이용하여 word2vec(skip-gram) 알고리즘을 구현해보는 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 PTB 데이터셋은 [여기](https://github.com/tomsercu/lstm)에서 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\ptb.train.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH =  Path('data')\n",
    "File_name = 'ptb.train.txt'\n",
    "print(DATA_PATH / File_name) # Path 설정 (운영체제 별로 / \\ 혼용되기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(DATA_PATH / File_name).read().replace('\\n','<eos>').strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has',\n",
       " 'been',\n",
       " 'less',\n",
       " 'prominent',\n",
       " 'according',\n",
       " 'to',\n",
       " 'mr.',\n",
       " '<unk>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[929580:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Look-up 테이블과 단어장을 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in corpus:\n",
    "    if word not in word_to_id:\n",
    "        tmp_id = len(word_to_id)\n",
    "        word_to_id[word]= tmp_id\n",
    "        id_to_word[tmp_id]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNIT = 100\n",
    "WINDOW_SIZE = 5\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus를 n-그램으로 쪼개자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer', 'banknote']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[corpus[0]]+[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = []\n",
    "center_words = []\n",
    "for i in range(WINDOW_SIZE, len(corpus)-WINDOW_SIZE):\n",
    "    context.append([corpus[i-j-1] for j in range(WINDOW_SIZE)]+\n",
    "                             [corpus[i+j+1] for j in range(WINDOW_SIZE) ])\n",
    "    center_words.append(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cluett',\n",
       " ['centrust',\n",
       "  'calloway',\n",
       "  'berlitz',\n",
       "  'banknote',\n",
       "  'aer',\n",
       "  'fromstein',\n",
       "  'gitano',\n",
       "  'guterman',\n",
       "  'hydro-quebec',\n",
       "  'ipo'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_words[0],context[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Center words와 Context words를 벡터화하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id_f(word):\n",
    "    return word_to_id[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_vec = []\n",
    "for cen in center_words:\n",
    "    cent_vec.append(word_to_id[cen])\n",
    "cent_vec = torch.tensor(cent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vec = []\n",
    "for con in context:\n",
    "    mp = map(word_to_id_f, con)\n",
    "    vec = [i for i in mp]\n",
    "    cont_vec.append(vec)\n",
    "cont_vec = torch.tensor(cont_vec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  3,  2,  1,  0,  6,  7,  8,  9, 10]), tensor(5), 929579)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_vec[0], cent_vec[0], len(cent_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Bag of Words 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW model\n",
    "class Word2vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab=vocab, hidden_unit = HIDDEN_UNIT, window_size=WINDOW_SIZE):\n",
    "        super(Word2vec, self).__init__()\n",
    "\n",
    "        self.size = len(vocab)\n",
    "        self.hidden_unit = hidden_unit\n",
    "        self.window_size = window_size\n",
    "        self.W1 = nn.Parameter(torch.randn(self.size, self.hidden_unit))\n",
    "        self.W2 = nn.Parameter(torch.randn(self.hidden_unit, self.size))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            vec = torch.zeros((1,self.size))\n",
    "            for j in range(2*self.window_size):\n",
    "                vec[0,x[j]]=1\n",
    "                \n",
    "        else:\n",
    "            vec = torch.zeros((len(x),self.size))\n",
    "            for i in range(len(x)):\n",
    "                for j in range(2*self.window_size):\n",
    "                    vec[i,x[i,j]]=1\n",
    "                \n",
    "        sample = (vec@self.W1)/(2*self.window_size)\n",
    "        pred = sample@self.W2\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec(cont_vec[0]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('W1', Parameter containing:\n",
      "tensor([[-0.9751,  1.0305, -1.9175,  ...,  0.0443, -1.9616, -0.1491],\n",
      "        [-1.8496,  0.2325,  1.1690,  ...,  0.8303,  0.3290,  0.4017],\n",
      "        [-1.7079, -0.4436,  0.1540,  ..., -2.0649, -1.2608,  0.0861],\n",
      "        ...,\n",
      "        [-0.1758, -0.1990,  0.5746,  ..., -0.7732,  0.8972,  0.3080],\n",
      "        [-1.0515,  1.3140, -0.2354,  ...,  0.8983, -0.8010, -1.0314],\n",
      "        [-0.1126, -0.1586, -0.4540,  ..., -0.9489, -0.7039, -1.1548]],\n",
      "       requires_grad=True))\n",
      "('W2', Parameter containing:\n",
      "tensor([[ 0.0434, -0.1767, -0.2205,  ...,  1.3001,  0.3414,  2.4879],\n",
      "        [ 0.6529, -0.8231, -0.7129,  ...,  0.2034,  0.4562,  1.8148],\n",
      "        [-0.3662,  1.5896, -1.4049,  ..., -1.7577, -1.6136,  1.0697],\n",
      "        ...,\n",
      "        [ 0.6154,  0.2480,  1.7762,  ..., -0.0784, -1.0135, -0.4529],\n",
      "        [-0.4520,  1.3719,  1.1661,  ...,  0.4912,  0.0528,  1.1597],\n",
      "        [ 1.2228,  0.4209, -0.1258,  ...,  1.1939,  1.0815, -0.1474]],\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(word2vec.named_parameters()):\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수는 크로스 엔트로피, 최적화 알고리즘은 SGD 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "optim = torch.optim.SGD(word2vec.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word2vec(cont_vec[0])\n",
    "y = cent_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.2806, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(word2vec(cont_vec[0]),  cent_vec[0].view(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 훈련하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1000 tensor(13.2573, grad_fn=<DivBackward0>)\n",
      "1 2000 tensor(13.0176, grad_fn=<DivBackward0>)\n",
      "1 3000 tensor(12.8929, grad_fn=<DivBackward0>)\n",
      "1 4000 tensor(12.6945, grad_fn=<DivBackward0>)\n",
      "1 5000 tensor(12.5660, grad_fn=<DivBackward0>)\n",
      "1 6000 tensor(12.3469, grad_fn=<DivBackward0>)\n",
      "1 7000 tensor(12.2688, grad_fn=<DivBackward0>)\n",
      "1 8000 tensor(12.0607, grad_fn=<DivBackward0>)\n",
      "1 9000 tensor(12.0058, grad_fn=<DivBackward0>)\n",
      "1 10000 tensor(11.8761, grad_fn=<DivBackward0>)\n",
      "1 11000 tensor(11.9002, grad_fn=<DivBackward0>)\n",
      "1 12000 tensor(11.7814, grad_fn=<DivBackward0>)\n",
      "1 13000 tensor(11.6586, grad_fn=<DivBackward0>)\n",
      "1 14000 tensor(11.5322, grad_fn=<DivBackward0>)\n",
      "2 1000 tensor(11.5925, grad_fn=<DivBackward0>)\n",
      "2 2000 tensor(11.4488, grad_fn=<DivBackward0>)\n",
      "2 3000 tensor(11.5197, grad_fn=<DivBackward0>)\n",
      "2 4000 tensor(11.4238, grad_fn=<DivBackward0>)\n",
      "2 5000 tensor(11.4039, grad_fn=<DivBackward0>)\n",
      "2 6000 tensor(11.2644, grad_fn=<DivBackward0>)\n",
      "2 7000 tensor(11.2712, grad_fn=<DivBackward0>)\n",
      "2 8000 tensor(11.1073, grad_fn=<DivBackward0>)\n",
      "2 9000 tensor(11.1125, grad_fn=<DivBackward0>)\n",
      "2 10000 tensor(11.0680, grad_fn=<DivBackward0>)\n",
      "2 11000 tensor(11.1291, grad_fn=<DivBackward0>)\n",
      "2 12000 tensor(11.0414, grad_fn=<DivBackward0>)\n",
      "2 13000 tensor(10.9494, grad_fn=<DivBackward0>)\n",
      "2 14000 tensor(10.8450, grad_fn=<DivBackward0>)\n",
      "3 1000 tensor(10.9722, grad_fn=<DivBackward0>)\n",
      "3 2000 tensor(10.8325, grad_fn=<DivBackward0>)\n",
      "3 3000 tensor(10.9324, grad_fn=<DivBackward0>)\n",
      "3 4000 tensor(10.8554, grad_fn=<DivBackward0>)\n",
      "3 5000 tensor(10.8554, grad_fn=<DivBackward0>)\n",
      "3 6000 tensor(10.7274, grad_fn=<DivBackward0>)\n",
      "3 7000 tensor(10.7511, grad_fn=<DivBackward0>)\n",
      "3 8000 tensor(10.5921, grad_fn=<DivBackward0>)\n",
      "3 9000 tensor(10.5989, grad_fn=<DivBackward0>)\n",
      "3 10000 tensor(10.5901, grad_fn=<DivBackward0>)\n",
      "3 11000 tensor(10.6610, grad_fn=<DivBackward0>)\n",
      "3 12000 tensor(10.5819, grad_fn=<DivBackward0>)\n",
      "3 13000 tensor(10.4997, grad_fn=<DivBackward0>)\n",
      "3 14000 tensor(10.4041, grad_fn=<DivBackward0>)\n",
      "4 1000 tensor(10.5577, grad_fn=<DivBackward0>)\n",
      "4 2000 tensor(10.4173, grad_fn=<DivBackward0>)\n",
      "4 3000 tensor(10.5263, grad_fn=<DivBackward0>)\n",
      "4 4000 tensor(10.4590, grad_fn=<DivBackward0>)\n",
      "4 5000 tensor(10.4664, grad_fn=<DivBackward0>)\n",
      "4 6000 tensor(10.3448, grad_fn=<DivBackward0>)\n",
      "4 7000 tensor(10.3788, grad_fn=<DivBackward0>)\n",
      "4 8000 tensor(10.2232, grad_fn=<DivBackward0>)\n",
      "4 9000 tensor(10.2288, grad_fn=<DivBackward0>)\n",
      "4 10000 tensor(10.2404, grad_fn=<DivBackward0>)\n",
      "4 11000 tensor(10.3152, grad_fn=<DivBackward0>)\n",
      "4 12000 tensor(10.2431, grad_fn=<DivBackward0>)\n",
      "4 13000 tensor(10.1656, grad_fn=<DivBackward0>)\n",
      "4 14000 tensor(10.0743, grad_fn=<DivBackward0>)\n",
      "5 1000 tensor(10.2406, grad_fn=<DivBackward0>)\n",
      "5 2000 tensor(10.1009, grad_fn=<DivBackward0>)\n",
      "5 3000 tensor(10.2125, grad_fn=<DivBackward0>)\n",
      "5 4000 tensor(10.1543, grad_fn=<DivBackward0>)\n",
      "5 5000 tensor(10.1642, grad_fn=<DivBackward0>)\n",
      "5 6000 tensor(10.0472, grad_fn=<DivBackward0>)\n",
      "5 7000 tensor(10.0890, grad_fn=<DivBackward0>)\n",
      "5 8000 tensor(9.9366, grad_fn=<DivBackward0>)\n",
      "5 9000 tensor(9.9401, grad_fn=<DivBackward0>)\n",
      "5 10000 tensor(9.9660, grad_fn=<DivBackward0>)\n",
      "5 11000 tensor(10.0411, grad_fn=<DivBackward0>)\n",
      "5 12000 tensor(9.9753, grad_fn=<DivBackward0>)\n",
      "5 13000 tensor(9.9012, grad_fn=<DivBackward0>)\n",
      "5 14000 tensor(9.8119, grad_fn=<DivBackward0>)\n",
      "Training time :67m 0s \n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_s = 0\n",
    "    for i in range((len(cont_vec) - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        \n",
    "        xb = cont_vec[start_i:end_i]\n",
    "        yb = cent_vec[start_i:end_i]\n",
    "        \n",
    "        pred = word2vec(xb)\n",
    "        \n",
    "        loss = loss_func(pred, yb)\n",
    "        loss_s += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        if i % 1000 == 999 :\n",
    "            print(epoch+1, i+1, loss_s/1000)\n",
    "            loss_s=0\n",
    "\n",
    "tt = round(time.time()-start_t)            \n",
    "print(\"Training time :\" + str(tt//60) + \"m \" + str(tt%60) + \"s \")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 결과를 일단 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(word2vec.state_dict(), 'cbow.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련된 단어 표현 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in word2vec.parameters():\n",
    "    W = copy.deepcopy(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 100])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어에 매핑 되는 벡터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vec(word):\n",
    "    return W[word_to_id[word]].view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3845e-01,  5.1904e-01,  1.3160e+00,  3.6017e-01,  8.1579e-02,\n",
       "          1.0051e+00,  8.5556e-02, -1.0812e+00,  5.5394e-01,  1.0792e+00,\n",
       "         -1.1921e+00,  1.6524e+00,  4.1688e-01,  6.4928e-01, -3.0679e-01,\n",
       "         -1.2483e+00, -1.1141e+00, -8.8036e-01, -6.0241e-01, -2.1682e-01,\n",
       "          4.4941e-01, -1.5590e+00, -4.0219e-01,  7.6776e-01,  1.1388e+00,\n",
       "          1.1669e+00,  9.2264e-01,  1.4978e+00,  2.5769e-01,  1.0776e-01,\n",
       "          1.2982e-03, -8.8100e-01,  2.4780e-01, -7.0344e-01,  9.7442e-01,\n",
       "         -3.6403e-01,  1.6898e+00,  3.4519e-01, -8.1621e-02,  5.4540e-01,\n",
       "         -6.8370e-01, -2.5154e-01, -2.4793e-01,  3.0588e-02, -3.6264e-01,\n",
       "          4.4998e-01,  5.2382e-02, -8.4516e-01,  2.3926e-01,  2.6648e-01,\n",
       "         -7.6238e-01, -5.1847e-01,  1.9922e+00, -6.5274e-01, -3.4827e-01,\n",
       "         -9.9975e-01, -2.9840e-01,  4.0354e-01,  5.3319e-01,  1.7421e+00,\n",
       "         -4.6960e-01, -8.2118e-01,  4.4036e-01,  5.7897e-01,  2.4332e-01,\n",
       "         -3.4287e-01,  5.0929e-01,  3.0784e-02,  1.2877e+00, -1.4054e+00,\n",
       "         -1.6039e+00,  9.6342e-01, -6.2292e-01, -3.8411e-01, -1.4261e-02,\n",
       "          6.7053e-01,  4.6357e-01,  6.8891e-01,  1.4090e+00,  8.5517e-01,\n",
       "          2.5495e-01, -3.2732e-01,  2.0043e+00,  2.3445e-01,  2.6556e+00,\n",
       "         -1.1094e+00, -1.8861e+00, -1.9418e+00, -1.7949e+00, -7.0475e-01,\n",
       "         -1.9731e-01, -5.6120e-01, -1.3447e+00,  8.4924e-01, -1.0166e+00,\n",
       "          5.7531e-01, -1.5341e-01,  2.2378e-01, -1.0692e+00, -6.8288e-01]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, m, q,w  = word_to_vec('king'), word_to_vec('man'), word_to_vec('queen'), word_to_vec('woman')\n",
    "ans = k -m + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(x,y):\n",
    "    return (torch.sum(x*y) / torch.sqrt((torch.sum(x*x))*(torch.sum(y*y)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 100])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_m = torch.zeros_like(W)\n",
    "ans_m.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 단어에 대한 cosine similarity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_m = [cos_sim(ans, W[i]) for i in range(len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.031105658039450645,\n",
       " -0.06424856185913086,\n",
       " 0.009941181167960167,\n",
       " -0.005522926338016987,\n",
       " 0.11785838007926941,\n",
       " -0.017450157552957535,\n",
       " -0.009432808496057987,\n",
       " 0.11596625298261642,\n",
       " -0.05872269347310066,\n",
       " 0.03671529144048691,\n",
       " -0.006257044617086649,\n",
       " -0.17627476155757904,\n",
       " -0.14356066286563873,\n",
       " 0.04618173837661743,\n",
       " -0.06440877914428711,\n",
       " -0.10119396448135376,\n",
       " -0.26191210746765137,\n",
       " 0.06462448835372925,\n",
       " 0.012532428838312626,\n",
       " -0.011429478414356709,\n",
       " 0.024043846875429153,\n",
       " -0.09419605880975723,\n",
       " -0.08623280376195908,\n",
       " 0.06988555192947388,\n",
       " -0.14107777178287506,\n",
       " 0.1118343397974968,\n",
       " -0.1431577503681183,\n",
       " -0.02995377592742443,\n",
       " 0.09832093119621277,\n",
       " 0.034563545137643814,\n",
       " 0.12666448950767517,\n",
       " -0.08430101722478867,\n",
       " -0.057016659528017044,\n",
       " 0.05489868298172951,\n",
       " 0.12376685440540314,\n",
       " -0.1346730887889862,\n",
       " 0.07886438071727753,\n",
       " 0.11971766501665115,\n",
       " -0.11823081225156784,\n",
       " -0.06040956825017929,\n",
       " 0.02661706507205963,\n",
       " 0.020304124802350998,\n",
       " -0.0055317506194114685,\n",
       " 0.09228567034006119,\n",
       " -0.11541497707366943,\n",
       " 0.09384319186210632,\n",
       " -0.023983703926205635,\n",
       " 0.09867209196090698,\n",
       " -0.16548573970794678,\n",
       " 0.0339636467397213,\n",
       " 0.08702807128429413,\n",
       " -0.17469531297683716,\n",
       " -0.1624208688735962,\n",
       " -0.06961870193481445,\n",
       " 0.09637719392776489,\n",
       " 0.16659051179885864,\n",
       " -0.14380356669425964,\n",
       " 0.04086729884147644,\n",
       " -0.08675866574048996,\n",
       " 0.04646110534667969,\n",
       " -0.006262727547436953,\n",
       " -0.1116684079170227,\n",
       " 0.03802057355642319,\n",
       " -0.005439923610538244,\n",
       " 0.14879624545574188,\n",
       " -0.0818396508693695,\n",
       " 0.007487752940505743,\n",
       " -0.023229848593473434,\n",
       " 0.07841799408197403,\n",
       " -0.30147260427474976,\n",
       " 0.002814823528751731,\n",
       " 0.030146291479468346,\n",
       " 0.019828608259558678,\n",
       " 0.05065711587667465,\n",
       " 0.0036447879392653704,\n",
       " -0.044268734753131866,\n",
       " 0.02813173271715641,\n",
       " 0.10206568241119385,\n",
       " 0.1864442676305771,\n",
       " -0.1277499496936798,\n",
       " 0.08653262257575989,\n",
       " 0.10684184730052948,\n",
       " 0.09606657922267914,\n",
       " 0.060693252831697464,\n",
       " 0.11223279684782028,\n",
       " -0.10220345854759216,\n",
       " 0.051772624254226685,\n",
       " 0.0493539497256279,\n",
       " -0.137150377035141,\n",
       " 0.02853970229625702,\n",
       " -0.013397423550486565,\n",
       " -0.03811216354370117,\n",
       " 0.05424652248620987,\n",
       " 0.04034196957945824,\n",
       " -0.03505305200815201,\n",
       " 0.010397698730230331,\n",
       " -0.020857438445091248,\n",
       " -0.020579520612955093,\n",
       " -0.15944595634937286,\n",
       " -0.01670989952981472,\n",
       " 0.08584417402744293,\n",
       " -0.004092267714440823,\n",
       " -0.05258559063076973,\n",
       " -0.009692883118987083,\n",
       " -0.07931480556726456,\n",
       " -0.06943807750940323,\n",
       " -0.029720664024353027,\n",
       " -0.07122478634119034,\n",
       " 0.08521556109189987,\n",
       " 0.13013163208961487,\n",
       " -0.03202829137444496,\n",
       " -0.13735568523406982,\n",
       " 0.06254738569259644,\n",
       " 0.0063613299280405045,\n",
       " 0.16865268349647522,\n",
       " 0.0613834373652935,\n",
       " -0.02257486991584301,\n",
       " 0.16110149025917053,\n",
       " -0.06247842684388161,\n",
       " 0.014510875567793846,\n",
       " -0.005567329935729504,\n",
       " 0.12186847627162933,\n",
       " -0.26025712490081787,\n",
       " 0.024358730763196945,\n",
       " -0.017297888174653053,\n",
       " -0.14043056964874268,\n",
       " 0.05037466064095497,\n",
       " 0.1359228491783142,\n",
       " -0.07280708104372025,\n",
       " 0.1374848335981369,\n",
       " 0.00470759766176343,\n",
       " 0.06355670094490051,\n",
       " 0.019425196573138237,\n",
       " 0.00578340794891119,\n",
       " 0.03337901458144188,\n",
       " -0.14211036264896393,\n",
       " 0.08143351972103119,\n",
       " 0.029973646625876427,\n",
       " -0.07035759836435318,\n",
       " -0.03600165992975235,\n",
       " -0.07295167446136475,\n",
       " 0.12177987396717072,\n",
       " 0.045441050082445145,\n",
       " 0.12681786715984344,\n",
       " -0.03454595059156418,\n",
       " 0.08048510551452637,\n",
       " 0.03767954185605049,\n",
       " 0.018346412107348442,\n",
       " -0.15411408245563507,\n",
       " -0.06950895488262177,\n",
       " 0.06203822046518326,\n",
       " -0.04480288550257683,\n",
       " -0.10946805030107498,\n",
       " 0.15980571508407593,\n",
       " -0.006274381652474403,\n",
       " 0.029281875118613243,\n",
       " 0.0769287720322609,\n",
       " -0.28331151604652405,\n",
       " -0.21150842308998108,\n",
       " 0.04089368134737015,\n",
       " 0.06699161231517792,\n",
       " 0.13608773052692413,\n",
       " 0.02922925166785717,\n",
       " -0.11518827825784683,\n",
       " 0.004197802860289812,\n",
       " -0.02640892192721367,\n",
       " -0.02182713709771633,\n",
       " 0.08857095241546631,\n",
       " -0.0809049978852272,\n",
       " 0.07405106723308563,\n",
       " -0.018983593210577965,\n",
       " -0.06605033576488495,\n",
       " -0.04222491756081581,\n",
       " -0.03829219564795494,\n",
       " 0.1266220211982727,\n",
       " 0.0816798210144043,\n",
       " -0.00843782164156437,\n",
       " 0.05811122804880142,\n",
       " -0.010331685654819012,\n",
       " -0.1678280532360077,\n",
       " -0.07491979748010635,\n",
       " -0.005107443314045668,\n",
       " 0.053884830325841904,\n",
       " -0.0427689254283905,\n",
       " 0.10905024409294128,\n",
       " -0.0035093831829726696,\n",
       " -0.1298130452632904,\n",
       " -0.10433939099311829,\n",
       " 0.005682123359292746,\n",
       " 0.0567391999065876,\n",
       " -0.05640975385904312,\n",
       " -0.1068425178527832,\n",
       " 0.0010338588617742062,\n",
       " 0.1447923481464386,\n",
       " -0.23334717750549316,\n",
       " 0.14326801896095276,\n",
       " -0.15937303006649017,\n",
       " 0.018865328282117844,\n",
       " 0.05624435842037201,\n",
       " 0.17542269825935364,\n",
       " 0.023852284997701645,\n",
       " 0.02109607867896557,\n",
       " 0.16006208956241608,\n",
       " -0.034450381994247437,\n",
       " -0.0017608879134058952,\n",
       " -0.03807574138045311,\n",
       " 0.04565022513270378,\n",
       " 0.07344722002744675,\n",
       " -0.07250536978244781,\n",
       " 0.04158288612961769,\n",
       " 0.2138012945652008,\n",
       " 0.1624106913805008,\n",
       " 0.03855454921722412,\n",
       " 0.1355227828025818,\n",
       " 0.08431487530469894,\n",
       " -0.0623985193669796,\n",
       " -0.1016554981470108,\n",
       " -0.04600023850798607,\n",
       " -0.0646776556968689,\n",
       " 0.018074674531817436,\n",
       " -0.15678556263446808,\n",
       " 0.08749300241470337,\n",
       " 0.072324737906456,\n",
       " -0.07168473303318024,\n",
       " -0.044030170887708664,\n",
       " 0.005141089670360088,\n",
       " -0.10790029913187027,\n",
       " -0.02771368995308876,\n",
       " 0.09942270815372467,\n",
       " 0.031576793640851974,\n",
       " 0.0738402009010315,\n",
       " -0.11925608664751053,\n",
       " -0.0006772293709218502,\n",
       " 0.04762418940663338,\n",
       " -0.0740213692188263,\n",
       " -0.11252384632825851,\n",
       " 0.19465266168117523,\n",
       " 0.01584012433886528,\n",
       " 0.07338793575763702,\n",
       " -0.022927118465304375,\n",
       " -0.12389736622571945,\n",
       " 0.06812430918216705,\n",
       " 0.005319807678461075,\n",
       " 0.19235138595104218,\n",
       " 0.007774699479341507,\n",
       " 0.026902280747890472,\n",
       " -0.23441347479820251,\n",
       " -0.09102654457092285,\n",
       " -0.00848967395722866,\n",
       " -0.17483815550804138,\n",
       " 0.028379835188388824,\n",
       " -0.06826261430978775,\n",
       " -0.050336387008428574,\n",
       " -0.14893439412117004,\n",
       " 0.09667947888374329,\n",
       " -0.03150438517332077,\n",
       " -0.07300392538309097,\n",
       " 0.11250688135623932,\n",
       " 0.09252879023551941,\n",
       " -0.17263396084308624,\n",
       " 0.03415065258741379,\n",
       " 0.14165690541267395,\n",
       " -0.023978371173143387,\n",
       " 0.04916287958621979,\n",
       " 0.023964067921042442,\n",
       " -0.04201152175664902,\n",
       " 0.014334440231323242,\n",
       " -0.01564708724617958,\n",
       " -0.1349947154521942,\n",
       " -0.12900295853614807,\n",
       " -0.0792636051774025,\n",
       " 0.07684167474508286,\n",
       " -0.04014185070991516,\n",
       " -0.0992639809846878,\n",
       " -0.048405036330223083,\n",
       " 0.08714690804481506,\n",
       " 0.051891569048166275,\n",
       " -0.2066415697336197,\n",
       " 0.11254671961069107,\n",
       " 0.02740996889770031,\n",
       " -0.1487778276205063,\n",
       " -0.11226564645767212,\n",
       " -0.0013610201422125101,\n",
       " -0.06349735707044601,\n",
       " -0.046409811824560165,\n",
       " 0.0766359493136406,\n",
       " 0.0396478995680809,\n",
       " 0.09735969454050064,\n",
       " 0.16009396314620972,\n",
       " -0.009912781417369843,\n",
       " 0.13380582630634308,\n",
       " -0.0567660890519619,\n",
       " 0.03986617177724838,\n",
       " -0.10729649662971497,\n",
       " -0.13766606152057648,\n",
       " -0.00557273905724287,\n",
       " 0.14417465031147003,\n",
       " -0.059503741562366486,\n",
       " 0.07018695771694183,\n",
       " 0.19184862077236176,\n",
       " 0.13347357511520386,\n",
       " 0.09632638841867447,\n",
       " -0.06843085587024689,\n",
       " -0.07386337965726852,\n",
       " 0.08474768698215485,\n",
       " 0.06549263000488281,\n",
       " -0.026826471090316772,\n",
       " -0.1723877340555191,\n",
       " -0.02320997789502144,\n",
       " -0.032095011323690414,\n",
       " 0.09720327705144882,\n",
       " -0.08323490619659424,\n",
       " -0.20677657425403595,\n",
       " 0.038808178156614304,\n",
       " 0.0774378553032875,\n",
       " -0.10362596809864044,\n",
       " -0.13735634088516235,\n",
       " 0.01798112317919731,\n",
       " -0.12091614305973053,\n",
       " -0.015872903168201447,\n",
       " 0.11797849833965302,\n",
       " -0.1268743872642517,\n",
       " 0.0472838394343853,\n",
       " -0.06552394479513168,\n",
       " 0.0008413307368755341,\n",
       " -0.10950297862291336,\n",
       " -0.017562948167324066,\n",
       " -0.0025357946287840605,\n",
       " -0.009413594380021095,\n",
       " -0.16828200221061707,\n",
       " -0.04447907209396362,\n",
       " -0.08350002020597458,\n",
       " -0.002427862025797367,\n",
       " -0.011308416724205017,\n",
       " -0.05002894252538681,\n",
       " 0.0635632574558258,\n",
       " 0.02238219603896141,\n",
       " -0.012155153788626194,\n",
       " -0.20367251336574554,\n",
       " -0.08610709756612778,\n",
       " 0.04007088020443916,\n",
       " -0.03008117340505123,\n",
       " -0.1384006291627884,\n",
       " 0.035710714757442474,\n",
       " 0.047655828297138214,\n",
       " 0.015841295942664146,\n",
       " 0.01713505946099758,\n",
       " -0.07563857734203339,\n",
       " 0.08802361786365509,\n",
       " 0.16613085567951202,\n",
       " 0.06321447342634201,\n",
       " -0.050343964248895645,\n",
       " 0.15279194712638855,\n",
       " 0.09368781000375748,\n",
       " 0.1548488289117813,\n",
       " 0.17715419828891754,\n",
       " -0.12251719832420349,\n",
       " 0.14712049067020416,\n",
       " -0.04646934196352959,\n",
       " -0.007350908126682043,\n",
       " 0.0013557508355006576,\n",
       " 0.10624050348997116,\n",
       " 0.10327915102243423,\n",
       " 0.10223276913166046,\n",
       " 0.11767376214265823,\n",
       " 0.05917120352387428,\n",
       " 0.05700933560729027,\n",
       " -0.050603512674570084,\n",
       " 0.10173998773097992,\n",
       " -0.0016320024151355028,\n",
       " 0.0481872633099556,\n",
       " 0.13088789582252502,\n",
       " 0.11703001707792282,\n",
       " -0.1531844288110733,\n",
       " 0.158895805478096,\n",
       " 0.11467111855745316,\n",
       " -0.05556333810091019,\n",
       " 0.007969014346599579,\n",
       " -0.09573115408420563,\n",
       " -0.0343007929623127,\n",
       " -0.045915428549051285,\n",
       " 0.001601200900040567,\n",
       " -0.09097383916378021,\n",
       " -0.013087283819913864,\n",
       " 0.1713276207447052,\n",
       " -0.16395510733127594,\n",
       " 0.13678665459156036,\n",
       " -0.03795498609542847,\n",
       " -0.14447148144245148,\n",
       " 0.05875982716679573,\n",
       " -0.04544879123568535,\n",
       " -0.2415933907032013,\n",
       " 0.043298281729221344,\n",
       " -0.04373989254236221,\n",
       " 0.13112682104110718,\n",
       " 0.2051527351140976,\n",
       " 0.04216006025671959,\n",
       " 0.012237204238772392,\n",
       " -0.21517623960971832,\n",
       " -0.021828044205904007,\n",
       " -0.04276853799819946,\n",
       " 0.23354269564151764,\n",
       " 0.08749750256538391,\n",
       " 0.06124468892812729,\n",
       " 0.20979426801204681,\n",
       " -0.040077704936265945,\n",
       " -0.1088850200176239,\n",
       " -0.07746519148349762,\n",
       " -0.04361141845583916,\n",
       " -0.04648691788315773,\n",
       " -0.1274748593568802,\n",
       " 0.03925362601876259,\n",
       " -0.0038808134850114584,\n",
       " -0.07306012511253357,\n",
       " -0.016987213864922523,\n",
       " 0.012674933299422264,\n",
       " 0.032569438219070435,\n",
       " -0.04609124734997749,\n",
       " 0.1025763526558876,\n",
       " 0.0322759672999382,\n",
       " 0.11957229673862457,\n",
       " 0.04821973294019699,\n",
       " 0.0662483498454094,\n",
       " -0.02263934165239334,\n",
       " 0.054548610001802444,\n",
       " 0.04147601127624512,\n",
       " -0.040475033223629,\n",
       " -0.03395257517695427,\n",
       " 0.09663605690002441,\n",
       " 0.01950300857424736,\n",
       " -0.06742795556783676,\n",
       " -0.20862166583538055,\n",
       " -0.049338094890117645,\n",
       " 0.08550018072128296,\n",
       " -0.1440752148628235,\n",
       " 0.052574194967746735,\n",
       " -0.056422360241413116,\n",
       " 0.2841382920742035,\n",
       " 0.211378276348114,\n",
       " 0.0346405915915966,\n",
       " -0.10039281845092773,\n",
       " 0.14302504062652588,\n",
       " 0.023987913504242897,\n",
       " 0.02672327123582363,\n",
       " -0.04858461767435074,\n",
       " -0.03185759112238884,\n",
       " 0.1431134045124054,\n",
       " 0.1473233997821808,\n",
       " -0.01727483980357647,\n",
       " -0.05977710708975792,\n",
       " -0.0030379975214600563,\n",
       " -0.05260444059967995,\n",
       " -0.0759708434343338,\n",
       " -0.006512593477964401,\n",
       " -0.044830337166786194,\n",
       " -0.07657688111066818,\n",
       " -0.13936863839626312,\n",
       " -0.018720507621765137,\n",
       " 0.01872801035642624,\n",
       " 0.021329976618289948,\n",
       " 0.13543500006198883,\n",
       " -0.08139274269342422,\n",
       " 0.09089402109384537,\n",
       " -0.020833177492022514,\n",
       " -0.1928243786096573,\n",
       " 0.03030513785779476,\n",
       " 0.05270681530237198,\n",
       " 0.09596932679414749,\n",
       " -0.10611552745103836,\n",
       " 0.19048403203487396,\n",
       " -0.09889266639947891,\n",
       " -0.14261497557163239,\n",
       " -0.04727143421769142,\n",
       " 0.162714421749115,\n",
       " -0.10413575917482376,\n",
       " -0.05580960959196091,\n",
       " 0.04291640967130661,\n",
       " 0.030604923143982887,\n",
       " -0.14313960075378418,\n",
       " -0.14479412138462067,\n",
       " -0.1711244434118271,\n",
       " 0.25655990839004517,\n",
       " 0.01969972811639309,\n",
       " -0.11281316727399826,\n",
       " -0.03691348806023598,\n",
       " 0.033193107694387436,\n",
       " 0.07173491269350052,\n",
       " -0.05291587859392166,\n",
       " 0.0320010781288147,\n",
       " -0.27582094073295593,\n",
       " -0.061405010521411896,\n",
       " -0.23588114976882935,\n",
       " -0.05272391811013222,\n",
       " -0.05771123617887497,\n",
       " -0.09100417047739029,\n",
       " 0.09542946517467499,\n",
       " -0.23313060402870178,\n",
       " 0.04540978744626045,\n",
       " -0.022733140736818314,\n",
       " 0.08581080287694931,\n",
       " -0.006713952403515577,\n",
       " 0.0352192148566246,\n",
       " -0.0486631914973259,\n",
       " -0.10934680700302124,\n",
       " -0.13514111936092377,\n",
       " -0.13562613725662231,\n",
       " 0.11679520457983017,\n",
       " -0.01722477190196514,\n",
       " 0.001164105604402721,\n",
       " 0.03999290615320206,\n",
       " 0.022334476932883263,\n",
       " -0.08711845427751541,\n",
       " 0.04128558188676834,\n",
       " 0.07853342592716217,\n",
       " -0.05332542210817337,\n",
       " 0.1145310029387474,\n",
       " 0.07398437708616257,\n",
       " -0.02899000234901905,\n",
       " -0.08325157314538956,\n",
       " 0.012789493426680565,\n",
       " -0.053807295858860016,\n",
       " 0.11645731329917908,\n",
       " -0.042141854763031006,\n",
       " -0.11050267517566681,\n",
       " 0.03482294827699661,\n",
       " 0.10999562591314316,\n",
       " 0.2522413432598114,\n",
       " -0.16568884253501892,\n",
       " -0.1348351687192917,\n",
       " 0.2271217554807663,\n",
       " -0.10344661772251129,\n",
       " 0.07811062783002853,\n",
       " 0.07119877636432648,\n",
       " 0.043394699692726135,\n",
       " 0.21849752962589264,\n",
       " 0.05088990926742554,\n",
       " 0.20238494873046875,\n",
       " 0.041501984000205994,\n",
       " -0.1140529215335846,\n",
       " 0.07480337470769882,\n",
       " -0.12764661014080048,\n",
       " 0.08895082771778107,\n",
       " -0.11191581934690475,\n",
       " 0.16766135394573212,\n",
       " 0.11476591974496841,\n",
       " -0.1438094526529312,\n",
       " -0.0085667809471488,\n",
       " -0.02493623085319996,\n",
       " -0.09206218272447586,\n",
       " 0.0308575090020895,\n",
       " 0.024364784359931946,\n",
       " -0.004972175229340792,\n",
       " -0.16622841358184814,\n",
       " 0.14260713756084442,\n",
       " -0.1512203961610794,\n",
       " 0.04710320383310318,\n",
       " -0.277683287858963,\n",
       " 0.2706514596939087,\n",
       " -0.104998879134655,\n",
       " -0.11509410291910172,\n",
       " -0.027175787836313248,\n",
       " 0.03197634592652321,\n",
       " -0.05798744037747383,\n",
       " -0.037750493735075,\n",
       " -0.05203086510300636,\n",
       " 0.15156276524066925,\n",
       " 0.18919247388839722,\n",
       " -0.1408357173204422,\n",
       " -0.2552345097064972,\n",
       " -0.15949945151805878,\n",
       " -0.03876882418990135,\n",
       " -0.057045772671699524,\n",
       " 0.05948688089847565,\n",
       " 0.02187158726155758,\n",
       " -0.06300053745508194,\n",
       " 0.07499991357326508,\n",
       " -0.025843707844614983,\n",
       " -0.09108705818653107,\n",
       " -0.16049815714359283,\n",
       " -0.090441033244133,\n",
       " 0.09611136466264725,\n",
       " 0.01128837838768959,\n",
       " 0.08622610569000244,\n",
       " -0.08477476984262466,\n",
       " -0.03243168443441391,\n",
       " 0.1904403269290924,\n",
       " -0.010792906396090984,\n",
       " -0.0314880833029747,\n",
       " 0.10358329862356186,\n",
       " 0.08148734271526337,\n",
       " -0.10275895893573761,\n",
       " -0.009183773770928383,\n",
       " 0.13287106156349182,\n",
       " -0.019643139094114304,\n",
       " -0.17536818981170654,\n",
       " -0.020145604386925697,\n",
       " 0.04855731874704361,\n",
       " -0.1380532830953598,\n",
       " 0.04940856248140335,\n",
       " 0.00991084799170494,\n",
       " -0.03748641163110733,\n",
       " 0.0019426221260800958,\n",
       " -0.04964296147227287,\n",
       " 0.23280218243598938,\n",
       " 0.23822417855262756,\n",
       " -0.08027942478656769,\n",
       " -0.08284942060709,\n",
       " 0.11446690559387207,\n",
       " 0.04627823829650879,\n",
       " -0.029838867485523224,\n",
       " 0.06679084151983261,\n",
       " 0.05529007315635681,\n",
       " 0.08894603699445724,\n",
       " 0.06267238408327103,\n",
       " -0.17884701490402222,\n",
       " -0.08841423690319061,\n",
       " -0.020559044554829597,\n",
       " 0.08564434945583344,\n",
       " -0.08270184695720673,\n",
       " 0.06786838173866272,\n",
       " -0.1599031388759613,\n",
       " 0.09229589998722076,\n",
       " 0.08512377738952637,\n",
       " 0.05498608946800232,\n",
       " 0.1682889610528946,\n",
       " -0.09144967049360275,\n",
       " -0.07506141811609268,\n",
       " 0.18082408607006073,\n",
       " 0.07232080399990082,\n",
       " 0.28004220128059387,\n",
       " 0.008233089931309223,\n",
       " -0.08480318635702133,\n",
       " -0.05157383531332016,\n",
       " -0.22207948565483093,\n",
       " -0.2168131023645401,\n",
       " 0.06656250357627869,\n",
       " 0.05879637598991394,\n",
       " -0.09456848353147507,\n",
       " 0.03565666824579239,\n",
       " 0.09671482443809509,\n",
       " -0.22749938070774078,\n",
       " -0.10377158969640732,\n",
       " -0.004192607011646032,\n",
       " -0.09096433967351913,\n",
       " 0.12088388949632645,\n",
       " -0.023066917434334755,\n",
       " -0.07695475220680237,\n",
       " -0.01764787547290325,\n",
       " -0.09640847891569138,\n",
       " -0.012128954753279686,\n",
       " 0.04862568527460098,\n",
       " -0.03167698532342911,\n",
       " 0.2542102634906769,\n",
       " 0.024322493001818657,\n",
       " -0.05458018556237221,\n",
       " -0.05330991372466087,\n",
       " 0.04052114114165306,\n",
       " -0.0415506549179554,\n",
       " -0.15480032563209534,\n",
       " 0.02018705941736698,\n",
       " -0.04175199940800667,\n",
       " 0.09177877008914948,\n",
       " 0.12111251056194305,\n",
       " 0.004996837582439184,\n",
       " -0.04984629154205322,\n",
       " -0.05307136848568916,\n",
       " 0.03286183625459671,\n",
       " 0.006801318842917681,\n",
       " 0.0027628703974187374,\n",
       " 0.016573956236243248,\n",
       " -0.06062234565615654,\n",
       " -0.16352418065071106,\n",
       " 0.14998392760753632,\n",
       " -0.08336269110441208,\n",
       " -0.021150127053260803,\n",
       " -0.1654120832681656,\n",
       " 0.04881765693426132,\n",
       " -0.017018280923366547,\n",
       " 0.10914158821105957,\n",
       " -0.013004540465772152,\n",
       " 0.23683799803256989,\n",
       " 0.04489872232079506,\n",
       " 0.03034576028585434,\n",
       " 0.020383408293128014,\n",
       " -0.019634844735264778,\n",
       " 0.010252222418785095,\n",
       " 0.07709299772977829,\n",
       " 0.032895658165216446,\n",
       " 0.1429302990436554,\n",
       " 0.08477114140987396,\n",
       " -0.0668632760643959,\n",
       " -0.11228317022323608,\n",
       " 0.054404404014348984,\n",
       " -0.07885915040969849,\n",
       " -0.10278643667697906,\n",
       " -0.22057925164699554,\n",
       " 0.03705206885933876,\n",
       " 0.022241994738578796,\n",
       " 0.05617247149348259,\n",
       " 0.041396792978048325,\n",
       " -0.033880770206451416,\n",
       " -0.11347812414169312,\n",
       " -0.0794350802898407,\n",
       " -0.027821561321616173,\n",
       " -0.08515869826078415,\n",
       " 0.11519576609134674,\n",
       " -0.11261751502752304,\n",
       " 0.20303714275360107,\n",
       " -0.13300821185112,\n",
       " -0.13191120326519012,\n",
       " -0.005696931853890419,\n",
       " 0.03240249305963516,\n",
       " 0.048822347074747086,\n",
       " 0.09122871607542038,\n",
       " -0.15251092612743378,\n",
       " -0.13176701962947845,\n",
       " -0.056677334010601044,\n",
       " -0.08705706149339676,\n",
       " -0.1640760898590088,\n",
       " 0.037224967032670975,\n",
       " 0.12298264354467392,\n",
       " 0.053698766976594925,\n",
       " 0.03799233213067055,\n",
       " -0.013470071367919445,\n",
       " 0.06844592839479446,\n",
       " 0.013181894086301327,\n",
       " -0.11892562359571457,\n",
       " 0.06955351680517197,\n",
       " -0.10039632767438889,\n",
       " 0.05688950791954994,\n",
       " 0.1060696691274643,\n",
       " 0.04799125716090202,\n",
       " -0.08620468527078629,\n",
       " -0.07773241400718689,\n",
       " -0.16081957519054413,\n",
       " -0.0038478716742247343,\n",
       " 0.09966780245304108,\n",
       " -0.00888861995190382,\n",
       " 0.020635781809687614,\n",
       " -0.11386191844940186,\n",
       " -0.1700064241886139,\n",
       " -0.1253870129585266,\n",
       " 0.16553831100463867,\n",
       " -0.08440688997507095,\n",
       " -0.04076680913567543,\n",
       " -0.12511932849884033,\n",
       " 0.17028401792049408,\n",
       " 0.14951588213443756,\n",
       " 0.14769625663757324,\n",
       " 0.08575871586799622,\n",
       " -0.06295335292816162,\n",
       " 0.038295384496450424,\n",
       " 0.03564578294754028,\n",
       " 0.17213550209999084,\n",
       " -0.07178671658039093,\n",
       " -0.028249701485037804,\n",
       " 0.01992649957537651,\n",
       " -0.033859122544527054,\n",
       " -0.019183948636054993,\n",
       " 0.11818228662014008,\n",
       " 0.003034650580957532,\n",
       " 0.09187296032905579,\n",
       " -0.05818261578679085,\n",
       " 0.057836804538965225,\n",
       " -0.09279809892177582,\n",
       " 0.023607799783349037,\n",
       " -0.07189718633890152,\n",
       " -0.025950085371732712,\n",
       " -0.06751172244548798,\n",
       " -0.16274523735046387,\n",
       " 0.0002427653962513432,\n",
       " -0.04130399972200394,\n",
       " 0.016626829281449318,\n",
       " -0.035009145736694336,\n",
       " 0.09520497918128967,\n",
       " 0.07278855890035629,\n",
       " -0.19112084805965424,\n",
       " -0.023995596915483475,\n",
       " 0.05394230782985687,\n",
       " 0.02466471865773201,\n",
       " -0.024131713435053825,\n",
       " 0.01584772579371929,\n",
       " -0.0563373900949955,\n",
       " -0.07964585721492767,\n",
       " -0.07334058731794357,\n",
       " 0.10822660475969315,\n",
       " -0.06408574432134628,\n",
       " -0.013724295422434807,\n",
       " 0.09022816270589828,\n",
       " -0.011932337656617165,\n",
       " -0.009900913573801517,\n",
       " 0.12187320739030838,\n",
       " 0.032122693955898285,\n",
       " 0.036228492856025696,\n",
       " -0.007197328377515078,\n",
       " -0.004451269283890724,\n",
       " 0.09518979489803314,\n",
       " -0.10223337262868881,\n",
       " 0.057805631309747696,\n",
       " 0.05769749730825424,\n",
       " -0.20800988376140594,\n",
       " -0.05127262324094772,\n",
       " 0.08836445957422256,\n",
       " -0.013626386411488056,\n",
       " -0.08389970660209656,\n",
       " 0.12314911931753159,\n",
       " 0.014968367293477058,\n",
       " 0.06845182180404663,\n",
       " -0.03250179812312126,\n",
       " -0.10834071040153503,\n",
       " -0.03886445239186287,\n",
       " -0.0977388396859169,\n",
       " -0.22304965555667877,\n",
       " -0.08320091664791107,\n",
       " -0.010352996177971363,\n",
       " 0.18957555294036865,\n",
       " 0.17423273622989655,\n",
       " -0.002367588924244046,\n",
       " 0.05832957476377487,\n",
       " -0.19028393924236298,\n",
       " 0.038867197930812836,\n",
       " 0.06466216593980789,\n",
       " -0.052352335304021835,\n",
       " -0.0923362448811531,\n",
       " -0.12481310963630676,\n",
       " -0.1254584789276123,\n",
       " -0.018629323691129684,\n",
       " 0.04760490730404854,\n",
       " 0.07079008966684341,\n",
       " -0.06942850351333618,\n",
       " 0.03327767550945282,\n",
       " -0.08433791249990463,\n",
       " 0.007066845893859863,\n",
       " -0.05994253605604172,\n",
       " 0.13572901487350464,\n",
       " -0.03840959817171097,\n",
       " -0.06275749951601028,\n",
       " -0.10081080347299576,\n",
       " 0.026222271844744682,\n",
       " 0.08063320070505142,\n",
       " 0.10342217981815338,\n",
       " 0.04268554970622063,\n",
       " -0.018121426925063133,\n",
       " -0.14180782437324524,\n",
       " 0.06145501136779785,\n",
       " -0.06902898102998734,\n",
       " 0.04794064536690712,\n",
       " -0.19697324931621552,\n",
       " 0.10001189261674881,\n",
       " 0.13267925381660461,\n",
       " 0.1627356857061386,\n",
       " 0.10523798316717148,\n",
       " -0.07140777260065079,\n",
       " 0.0005734935402870178,\n",
       " 0.03676195070147514,\n",
       " 0.115081287920475,\n",
       " 0.0967174768447876,\n",
       " 0.18859164416790009,\n",
       " -0.04560258612036705,\n",
       " -0.07259707897901535,\n",
       " 0.10356832295656204,\n",
       " -0.01636248268187046,\n",
       " -0.07986165583133698,\n",
       " 0.03285948559641838,\n",
       " -0.25411128997802734,\n",
       " 0.18459324538707733,\n",
       " -0.14940327405929565,\n",
       " 0.02477140538394451,\n",
       " -0.02179615944623947,\n",
       " 0.20259664952754974,\n",
       " -0.13049723207950592,\n",
       " -0.13037365674972534,\n",
       " -0.056983817368745804,\n",
       " 0.18482820689678192,\n",
       " -0.28262439370155334,\n",
       " 0.01948055438697338,\n",
       " -0.04308454692363739,\n",
       " 0.0331885851919651,\n",
       " -0.12173476815223694,\n",
       " 0.005899412557482719,\n",
       " -0.08332320302724838,\n",
       " 0.026060588657855988,\n",
       " -0.04583703354001045,\n",
       " -0.03059825487434864,\n",
       " -0.024713482707738876,\n",
       " -0.015287510119378567,\n",
       " -0.08685140311717987,\n",
       " -0.05663770064711571,\n",
       " -0.0384247861802578,\n",
       " -0.10587047040462494,\n",
       " 0.19685646891593933,\n",
       " -0.05343262478709221,\n",
       " 0.03110368363559246,\n",
       " -0.18628302216529846,\n",
       " -0.11519341170787811,\n",
       " 0.0744544044137001,\n",
       " -0.13428662717342377,\n",
       " -0.1785808652639389,\n",
       " 0.21899060904979706,\n",
       " 0.10731212049722672,\n",
       " 0.13599753379821777,\n",
       " -0.06862179189920425,\n",
       " 0.04396417364478111,\n",
       " 0.12414080649614334,\n",
       " -0.1314471811056137,\n",
       " 0.10256822407245636,\n",
       " 0.05099603906273842,\n",
       " -0.04165008291602135,\n",
       " -0.08453094959259033,\n",
       " 0.07868005335330963,\n",
       " -0.1681768149137497,\n",
       " -0.01892988383769989,\n",
       " -0.03429970145225525,\n",
       " 0.11393047869205475,\n",
       " 0.05743061751127243,\n",
       " -0.08582718670368195,\n",
       " 0.12068378925323486,\n",
       " -0.12522676587104797,\n",
       " 0.05624879151582718,\n",
       " 0.07164233177900314,\n",
       " -0.04066094383597374,\n",
       " 0.1700308471918106,\n",
       " 0.16198964416980743,\n",
       " -0.015138436108827591,\n",
       " 0.15896077454090118,\n",
       " -0.028268255293369293,\n",
       " 0.050555966794490814,\n",
       " 0.0675613060593605,\n",
       " -0.15744857490062714,\n",
       " 0.090122289955616,\n",
       " -0.07754767686128616,\n",
       " 0.10705691576004028,\n",
       " 0.061715468764305115,\n",
       " -0.05104609206318855,\n",
       " 0.11766109615564346,\n",
       " -0.10713168978691101,\n",
       " -0.07665635645389557,\n",
       " -0.10906982421875,\n",
       " 0.07958492636680603,\n",
       " 0.02309124916791916,\n",
       " 0.05544867739081383,\n",
       " 0.14967279136180878,\n",
       " 0.1560485064983368,\n",
       " 0.034293219447135925,\n",
       " -0.1298714280128479,\n",
       " 0.14908969402313232,\n",
       " 0.15210942924022675,\n",
       " 0.06728283315896988,\n",
       " 0.0025118300691246986,\n",
       " -0.09630423784255981,\n",
       " 0.10390686243772507,\n",
       " -0.05380069464445114,\n",
       " -0.11854980140924454,\n",
       " -0.14307533204555511,\n",
       " 0.10683573782444,\n",
       " -0.16357432305812836,\n",
       " 0.04486774280667305,\n",
       " 0.05632073059678078,\n",
       " -0.1389770656824112,\n",
       " 0.029453711584210396,\n",
       " -0.0217061135917902,\n",
       " 0.08471320569515228,\n",
       " -0.08238591998815536,\n",
       " -0.1079857349395752,\n",
       " -0.042773205786943436,\n",
       " 0.16293971240520477,\n",
       " -0.05808352306485176,\n",
       " 0.0453399159014225,\n",
       " -0.17242860794067383,\n",
       " 0.05044935271143913,\n",
       " -0.025467367842793465,\n",
       " 0.16385717689990997,\n",
       " -0.17458297312259674,\n",
       " -0.22032047808170319,\n",
       " 0.06778810918331146,\n",
       " 0.053460150957107544,\n",
       " -0.0476190559566021,\n",
       " -0.03638150542974472,\n",
       " 0.01229962520301342,\n",
       " -0.10162332653999329,\n",
       " -0.20767050981521606,\n",
       " 0.10044112801551819,\n",
       " -0.03970829024910927,\n",
       " 0.049079738557338715,\n",
       " -0.0923822671175003,\n",
       " 0.013209822587668896,\n",
       " -0.011695177294313908,\n",
       " -0.0997285395860672,\n",
       " -0.17593887448310852,\n",
       " 0.020601846277713776,\n",
       " -0.007872577756643295,\n",
       " 0.1541243940591812,\n",
       " -0.1552964597940445,\n",
       " 0.07662783563137054,\n",
       " 0.0700894147157669,\n",
       " 0.08362985402345657,\n",
       " 0.014178367331624031,\n",
       " 0.06935543566942215,\n",
       " -0.005336386617273092,\n",
       " -0.029125111177563667,\n",
       " ...]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n",
      "king\n",
      "appearances\n",
      "construct\n",
      "costing\n"
     ]
    }
   ],
   "source": [
    "for i in np.array(ans_m).argsort()[-5:][::-1]:\n",
    "    print(id_to_word[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "망했다!\n",
    "\n",
    "이를 일반화해보자.\n",
    "\n",
    "$$ \\text{word}1 : \\text{word}2 \\simeq \\text{word}3 : \\text{word}4 $$\n",
    "\n",
    "라는 관계를 얻을 수 있는 지 유추하기 위해, $\\text{word}1, \\text{word}2 , \\text{word}4 $ 를 입력하면 $\\text{word}3$와 가장 유사한 상위 5개의 단어를 얻는 함수를 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_analogy(w1, w2, w4):\n",
    "    ans = word_to_vec(w1)-word_to_vec(w2)+word_to_vec(w4)\n",
    "    \n",
    "    ans_m = [cos_sim(ans, W[i]) for i in range(len(vocab))]\n",
    "    for i in np.array(ans_m).argsort()[-5:][::-1]:\n",
    "        print(id_to_word[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n",
      "king\n",
      "appearances\n",
      "construct\n",
      "costing\n"
     ]
    }
   ],
   "source": [
    "tri_analogy('king','man','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korea\n",
      "london\n",
      "garage\n",
      "suspected\n",
      "revise\n"
     ]
    }
   ],
   "source": [
    "tri_analogy('korea','seoul','london')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아무래도 거지같으니까 훈련을 더 해보자. 5에폭을 더 해서 총 10에폭을 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1000 tensor(9.9863, grad_fn=<DivBackward0>)\n",
      "1 2000 tensor(9.8478, grad_fn=<DivBackward0>)\n",
      "1 3000 tensor(9.9594, grad_fn=<DivBackward0>)\n",
      "1 4000 tensor(9.9103, grad_fn=<DivBackward0>)\n",
      "1 5000 tensor(9.9202, grad_fn=<DivBackward0>)\n",
      "1 6000 tensor(9.8065, grad_fn=<DivBackward0>)\n",
      "1 7000 tensor(9.8546, grad_fn=<DivBackward0>)\n",
      "1 8000 tensor(9.7052, grad_fn=<DivBackward0>)\n",
      "1 9000 tensor(9.7065, grad_fn=<DivBackward0>)\n",
      "1 10000 tensor(9.7433, grad_fn=<DivBackward0>)\n",
      "1 11000 tensor(9.8171, grad_fn=<DivBackward0>)\n",
      "1 12000 tensor(9.7568, grad_fn=<DivBackward0>)\n",
      "1 13000 tensor(9.6856, grad_fn=<DivBackward0>)\n",
      "1 14000 tensor(9.5978, grad_fn=<DivBackward0>)\n",
      "2 1000 tensor(9.7777, grad_fn=<DivBackward0>)\n",
      "2 2000 tensor(9.6405, grad_fn=<DivBackward0>)\n",
      "2 3000 tensor(9.7510, grad_fn=<DivBackward0>)\n",
      "2 4000 tensor(9.7103, grad_fn=<DivBackward0>)\n",
      "2 5000 tensor(9.7186, grad_fn=<DivBackward0>)\n",
      "2 6000 tensor(9.6076, grad_fn=<DivBackward0>)\n",
      "2 7000 tensor(9.6610, grad_fn=<DivBackward0>)\n",
      "2 8000 tensor(9.5140, grad_fn=<DivBackward0>)\n",
      "2 9000 tensor(9.5134, grad_fn=<DivBackward0>)\n",
      "2 10000 tensor(9.5590, grad_fn=<DivBackward0>)\n",
      "2 11000 tensor(9.6306, grad_fn=<DivBackward0>)\n",
      "2 12000 tensor(9.5750, grad_fn=<DivBackward0>)\n",
      "2 13000 tensor(9.5056, grad_fn=<DivBackward0>)\n",
      "2 14000 tensor(9.4198, grad_fn=<DivBackward0>)\n",
      "3 1000 tensor(9.6033, grad_fn=<DivBackward0>)\n",
      "3 2000 tensor(9.4672, grad_fn=<DivBackward0>)\n",
      "3 3000 tensor(9.5758, grad_fn=<DivBackward0>)\n",
      "3 4000 tensor(9.5428, grad_fn=<DivBackward0>)\n",
      "3 5000 tensor(9.5488, grad_fn=<DivBackward0>)\n",
      "3 6000 tensor(9.4405, grad_fn=<DivBackward0>)\n",
      "3 7000 tensor(9.4976, grad_fn=<DivBackward0>)\n",
      "3 8000 tensor(9.3526, grad_fn=<DivBackward0>)\n",
      "3 9000 tensor(9.3507, grad_fn=<DivBackward0>)\n",
      "3 10000 tensor(9.4035, grad_fn=<DivBackward0>)\n",
      "3 11000 tensor(9.4724, grad_fn=<DivBackward0>)\n",
      "3 12000 tensor(9.4205, grad_fn=<DivBackward0>)\n",
      "3 13000 tensor(9.3525, grad_fn=<DivBackward0>)\n",
      "3 14000 tensor(9.2689, grad_fn=<DivBackward0>)\n",
      "4 1000 tensor(9.4546, grad_fn=<DivBackward0>)\n",
      "4 2000 tensor(9.3196, grad_fn=<DivBackward0>)\n",
      "4 3000 tensor(9.4259, grad_fn=<DivBackward0>)\n",
      "4 4000 tensor(9.3998, grad_fn=<DivBackward0>)\n",
      "4 5000 tensor(9.4031, grad_fn=<DivBackward0>)\n",
      "4 6000 tensor(9.2976, grad_fn=<DivBackward0>)\n",
      "4 7000 tensor(9.3570, grad_fn=<DivBackward0>)\n",
      "4 8000 tensor(9.2138, grad_fn=<DivBackward0>)\n",
      "4 9000 tensor(9.2110, grad_fn=<DivBackward0>)\n",
      "4 10000 tensor(9.2699, grad_fn=<DivBackward0>)\n",
      "4 11000 tensor(9.3359, grad_fn=<DivBackward0>)\n",
      "4 12000 tensor(9.2870, grad_fn=<DivBackward0>)\n",
      "4 13000 tensor(9.2200, grad_fn=<DivBackward0>)\n",
      "4 14000 tensor(9.1383, grad_fn=<DivBackward0>)\n",
      "5 1000 tensor(9.3257, grad_fn=<DivBackward0>)\n",
      "5 2000 tensor(9.1917, grad_fn=<DivBackward0>)\n",
      "5 3000 tensor(9.2954, grad_fn=<DivBackward0>)\n",
      "5 4000 tensor(9.2756, grad_fn=<DivBackward0>)\n",
      "5 5000 tensor(9.2759, grad_fn=<DivBackward0>)\n",
      "5 6000 tensor(9.1734, grad_fn=<DivBackward0>)\n",
      "5 7000 tensor(9.2343, grad_fn=<DivBackward0>)\n",
      "5 8000 tensor(9.0925, grad_fn=<DivBackward0>)\n",
      "5 9000 tensor(9.0890, grad_fn=<DivBackward0>)\n",
      "5 10000 tensor(9.1531, grad_fn=<DivBackward0>)\n",
      "5 11000 tensor(9.2163, grad_fn=<DivBackward0>)\n",
      "5 12000 tensor(9.1698, grad_fn=<DivBackward0>)\n",
      "5 13000 tensor(9.1037, grad_fn=<DivBackward0>)\n",
      "5 14000 tensor(9.0235, grad_fn=<DivBackward0>)\n",
      "Training time :68m 32s \n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_s = 0\n",
    "    for i in range((len(cont_vec) - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        \n",
    "        xb = cont_vec[start_i:end_i]\n",
    "        yb = cent_vec[start_i:end_i]\n",
    "        \n",
    "        pred = word2vec(xb)\n",
    "        \n",
    "        loss = loss_func(pred, yb)\n",
    "        loss_s += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        if i % 1000 == 999 :\n",
    "            print(epoch+1, i+1, loss_s/1000)\n",
    "            loss_s=0\n",
    "\n",
    "tt = round(time.time()-start_t)            \n",
    "print(\"Training time :\" + str(tt//60) + \"m \" + str(tt%60) + \"s \")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(word2vec.state_dict(), 'cbow_10epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in word2vec.parameters():\n",
    "    W = copy.deepcopy(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n",
      "king\n",
      "appearances\n",
      "construct\n",
      "costing\n"
     ]
    }
   ],
   "source": [
    "tri_analogy('king','man','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korea\n",
      "london\n",
      "garage\n",
      "suspected\n",
      "revise\n"
     ]
    }
   ],
   "source": [
    "tri_analogy('korea','seoul','london')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개망"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
