{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Hi my name is ttt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x108e2f390>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x154588108>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x154588168>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipleline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:',nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hi my name is ttt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 2\n",
      "Doc length: 1\n",
      "Doc length: 1\n",
      "Doc length: 2\n",
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "\n",
    "\n",
    "\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns) # matcher의 패턴 입력은 tuple 아니면 doc만 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(animal_patterns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "tt = ['a','b','c']\n",
    "for t in tt: print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'spacy.tokens.doc.Doc'>, <class 'spacy.tokens.doc.Doc'>, <class 'spacy.tokens.doc.Doc'>, <class 'spacy.tokens.doc.Doc'>]\n"
     ]
    }
   ],
   "source": [
    "print([type(doc) for doc in animal_patterns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'spacy.tokens.doc.Doc'>, <class 'spacy.tokens.doc.Doc'>, <class 'spacy.tokens.doc.Doc'>, <class 'spacy.tokens.doc.Doc'>]\n"
     ]
    }
   ],
   "source": [
    "print([type(animal_patterns[i]) for i in range(len(animal_patterns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(animals), type(animal_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 6303828839600189595, 'ANIMAL'), ('Golden Retriever', 6303828839600189595, 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text,ent.label, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add custom metadata to documents, tokens and spans\n",
    "* Accsible via the `._` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1eeb369987a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "doc.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E047] Can't assign a value to unregistered extension attribute 'title'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-92751389d7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'My document'\u001b[0m \u001b[0;31m# 미리 사전 등록이 필요함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_ev/lib/python3.7/site-packages/spacy/tokens/underscore.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE047\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: [E047] Can't assign a value to unregistered extension attribute 'title'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "doc._.title = 'My document' # 미리 사전 등록이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My document\n"
     ]
    }
   ],
   "source": [
    "doc._.title = 'My document'\n",
    "print(doc._.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[0]\n",
    "span = doc[0:3]\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.is_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('The sky is blue.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extensions\n",
    "\n",
    "* Define a getter and an optional setter function\n",
    "* Getter only called when you retrieve the attribute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, None, None, None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Token.remove_extension('is_color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color', getter=get_is_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.remove_extension('has_color')\n",
    "Span.set_extension('has_color', getter=get_has_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method extensions\n",
    "\n",
    "* Assign a <b>function</b> that becomes available as an object method\n",
    "* Lets you pass <b>arguments</b> to the extension function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method with arguments\n",
    "def has_token(doc, token_text): # method 전달 시 첫번째가 해당 object, 그 이후가 함수로 전달하는 인자\n",
    "    return  token_text in [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method=has_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True  - blue\n",
      "False  - cloud\n"
     ]
    }
   ],
   "source": [
    "print(doc._.has_token('blue'), ' - blue')\n",
    "print(doc._.has_token('cloud'), ' - cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The sky is blue."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises : Setting extension attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txet'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = 'text'\n",
    "tt[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 9\n",
      "reversed All llA\n",
      "reversed generalizations snoitazilareneg\n",
      "reversed are era\n",
      "reversed false eslaf\n",
      "reversed , ,\n",
      "reversed including gnidulcni\n",
      "reversed this siht\n",
      "reversed one eno\n",
      "reversed . .\n"
     ]
    }
   ],
   "source": [
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "Token.set_extension('reversed',getter=get_reversed)\n",
    "\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed', token.text, token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 9\n",
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "doc = nlp('The museum closed for five years in 2012.')\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 8\n",
      "Hello world <strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return \"<{tag}>{text}</{tag}>\".format(tag=tag, text=span.text)\n",
    "\n",
    "Span.set_extension('to_html',method=to_html)\n",
    "\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span.text, span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises : Entities and extensions\n",
    "\n",
    "In this exercise, you'll combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location\n",
    "\n",
    "* Complete the `get_wikipedia_url` getter so it oly returns the URL if the span's label is in the list of labels.\n",
    "* Set the `span` extension `'wikipedia_url'` using the getter `get_wikipedia_url`.\n",
    "* Iterate over the entities in the `doc` and output their Wikipedia URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 27\n",
      "over fifty years DATE None\n",
      "first ORDINAL None\n",
      "David Bowie PERSON https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\"+entity_text\n",
    "    \n",
    "#Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \" in over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent.label_, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises : Components with extensions\n",
    "\n",
    "In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available.\n",
    "\n",
    "A phrase matcher with all countries is available as the variable `matcher`. A dictionary of countries mapped to their capital cities is available as the variable `CAPITALS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open('spaCy/exercises/countries.json') as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "    \n",
    "with open('spaCy/exercises/capitals.json') as f:\n",
    "    CAPITALS = json.loads(f.read())    \n",
    "    \n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "def countries_component(doc):\n",
    "    # Create an entitiy Span with the label 'GPE' for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Åland Islands',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'American Samoa',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Anguilla',\n",
       " 'Antarctica',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'Aruba',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Bahamas',\n",
       " 'Bahrain',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Belarus',\n",
       " 'Belgium',\n",
       " 'Belize',\n",
       " 'Benin',\n",
       " 'Bermuda',\n",
       " 'Bhutan',\n",
       " 'Bolivia (Plurinational State of)',\n",
       " 'Bonaire, Sint Eustatius and Saba',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Botswana',\n",
       " 'Bouvet Island',\n",
       " 'Brazil',\n",
       " 'British Indian Ocean Territory',\n",
       " 'United States Minor Outlying Islands',\n",
       " 'Virgin Islands (British)',\n",
       " 'Virgin Islands (U.S.)',\n",
       " 'Brunei Darussalam',\n",
       " 'Bulgaria',\n",
       " 'Burkina Faso',\n",
       " 'Burundi',\n",
       " 'Cambodia',\n",
       " 'Cameroon',\n",
       " 'Canada',\n",
       " 'Cabo Verde',\n",
       " 'Cayman Islands',\n",
       " 'Central African Republic',\n",
       " 'Chad',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Christmas Island',\n",
       " 'Cocos (Keeling) Islands',\n",
       " 'Colombia',\n",
       " 'Comoros',\n",
       " 'Congo',\n",
       " 'Congo (Democratic Republic of the)',\n",
       " 'Cook Islands',\n",
       " 'Costa Rica',\n",
       " 'Croatia',\n",
       " 'Cuba',\n",
       " 'Curaçao',\n",
       " 'Cyprus',\n",
       " 'Czech Republic',\n",
       " 'Denmark',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Dominican Republic',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'El Salvador',\n",
       " 'Equatorial Guinea',\n",
       " 'Eritrea',\n",
       " 'Estonia',\n",
       " 'Ethiopia',\n",
       " 'Falkland Islands (Malvinas)',\n",
       " 'Faroe Islands',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'France',\n",
       " 'French Guiana',\n",
       " 'French Polynesia',\n",
       " 'French Southern Territories',\n",
       " 'Gabon',\n",
       " 'Gambia',\n",
       " 'Georgia',\n",
       " 'Germany',\n",
       " 'Ghana',\n",
       " 'Gibraltar',\n",
       " 'Greece',\n",
       " 'Greenland',\n",
       " 'Grenada',\n",
       " 'Guadeloupe',\n",
       " 'Guam',\n",
       " 'Guatemala',\n",
       " 'Guernsey',\n",
       " 'Guinea',\n",
       " 'Guinea-Bissau',\n",
       " 'Guyana',\n",
       " 'Haiti',\n",
       " 'Heard Island and McDonald Islands',\n",
       " 'Holy See',\n",
       " 'Honduras',\n",
       " 'Hong Kong',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " \"Côte d'Ivoire\",\n",
       " 'Iran (Islamic Republic of)',\n",
       " 'Iraq',\n",
       " 'Ireland',\n",
       " 'Isle of Man',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Jamaica',\n",
       " 'Japan',\n",
       " 'Jersey',\n",
       " 'Jordan',\n",
       " 'Kazakhstan',\n",
       " 'Kenya',\n",
       " 'Kiribati',\n",
       " 'Kuwait',\n",
       " 'Kyrgyzstan',\n",
       " \"Lao People's Democratic Republic\",\n",
       " 'Latvia',\n",
       " 'Lebanon',\n",
       " 'Lesotho',\n",
       " 'Liberia',\n",
       " 'Libya',\n",
       " 'Liechtenstein',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Macao',\n",
       " 'Macedonia (the former Yugoslav Republic of)',\n",
       " 'Madagascar',\n",
       " 'Malawi',\n",
       " 'Malaysia',\n",
       " 'Maldives',\n",
       " 'Mali',\n",
       " 'Malta',\n",
       " 'Marshall Islands',\n",
       " 'Martinique',\n",
       " 'Mauritania',\n",
       " 'Mauritius',\n",
       " 'Mayotte',\n",
       " 'Mexico',\n",
       " 'Micronesia (Federated States of)',\n",
       " 'Moldova (Republic of)',\n",
       " 'Monaco',\n",
       " 'Mongolia',\n",
       " 'Montenegro',\n",
       " 'Montserrat',\n",
       " 'Morocco',\n",
       " 'Mozambique',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Nauru',\n",
       " 'Nepal',\n",
       " 'Netherlands',\n",
       " 'New Caledonia',\n",
       " 'New Zealand',\n",
       " 'Nicaragua',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'Niue',\n",
       " 'Norfolk Island',\n",
       " \"Korea (Democratic People's Republic of)\",\n",
       " 'Northern Mariana Islands',\n",
       " 'Norway',\n",
       " 'Oman',\n",
       " 'Pakistan',\n",
       " 'Palau',\n",
       " 'Palestine, State of',\n",
       " 'Panama',\n",
       " 'Papua New Guinea',\n",
       " 'Paraguay',\n",
       " 'Peru',\n",
       " 'Philippines',\n",
       " 'Pitcairn',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Puerto Rico',\n",
       " 'Qatar',\n",
       " 'Republic of Kosovo',\n",
       " 'Réunion',\n",
       " 'Romania',\n",
       " 'Russian Federation',\n",
       " 'Rwanda',\n",
       " 'Saint Barthélemy',\n",
       " 'Saint Helena, Ascension and Tristan da Cunha',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Saint Lucia',\n",
       " 'Saint Martin (French part)',\n",
       " 'Saint Pierre and Miquelon',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'Samoa',\n",
       " 'San Marino',\n",
       " 'Sao Tome and Principe',\n",
       " 'Saudi Arabia',\n",
       " 'Senegal',\n",
       " 'Serbia',\n",
       " 'Seychelles',\n",
       " 'Sierra Leone',\n",
       " 'Singapore',\n",
       " 'Sint Maarten (Dutch part)',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Solomon Islands',\n",
       " 'Somalia',\n",
       " 'South Africa',\n",
       " 'South Georgia and the South Sandwich Islands',\n",
       " 'Korea (Republic of)',\n",
       " 'South Sudan',\n",
       " 'Spain',\n",
       " 'Sri Lanka',\n",
       " 'Sudan',\n",
       " 'Suriname',\n",
       " 'Svalbard and Jan Mayen',\n",
       " 'Swaziland',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Syrian Arab Republic',\n",
       " 'Taiwan',\n",
       " 'Tajikistan',\n",
       " 'Tanzania, United Republic of',\n",
       " 'Thailand',\n",
       " 'Timor-Leste',\n",
       " 'Togo',\n",
       " 'Tokelau',\n",
       " 'Tonga',\n",
       " 'Trinidad and Tobago',\n",
       " 'Tunisia',\n",
       " 'Turkey',\n",
       " 'Turkmenistan',\n",
       " 'Turks and Caicos Islands',\n",
       " 'Tuvalu',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom of Great Britain and Northern Ireland',\n",
       " 'United States of America',\n",
       " 'Uruguay',\n",
       " 'Uzbekistan',\n",
       " 'Vanuatu',\n",
       " 'Venezuela (Bolivarian Republic of)',\n",
       " 'Viet Nam',\n",
       " 'Wallis and Futuna',\n",
       " 'Western Sahara',\n",
       " 'Yemen',\n",
       " 'Zambia',\n",
       " 'Zimbabwe']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Afghanistan': 'Kabul',\n",
       " 'Åland Islands': 'Mariehamn',\n",
       " 'Albania': 'Tirana',\n",
       " 'Algeria': 'Algiers',\n",
       " 'American Samoa': 'Pago Pago',\n",
       " 'Andorra': 'Andorra la Vella',\n",
       " 'Angola': 'Luanda',\n",
       " 'Anguilla': 'The Valley',\n",
       " 'Antarctica': '',\n",
       " 'Antigua and Barbuda': \"Saint John's\",\n",
       " 'Argentina': 'Buenos Aires',\n",
       " 'Armenia': 'Yerevan',\n",
       " 'Aruba': 'Oranjestad',\n",
       " 'Australia': 'Canberra',\n",
       " 'Austria': 'Vienna',\n",
       " 'Azerbaijan': 'Baku',\n",
       " 'Bahamas': 'Nassau',\n",
       " 'Bahrain': 'Manama',\n",
       " 'Bangladesh': 'Dhaka',\n",
       " 'Barbados': 'Bridgetown',\n",
       " 'Belarus': 'Minsk',\n",
       " 'Belgium': 'Brussels',\n",
       " 'Belize': 'Belmopan',\n",
       " 'Benin': 'Porto-Novo',\n",
       " 'Bermuda': 'Hamilton',\n",
       " 'Bhutan': 'Thimphu',\n",
       " 'Bolivia (Plurinational State of)': 'Sucre',\n",
       " 'Bonaire, Sint Eustatius and Saba': 'Kralendijk',\n",
       " 'Bosnia and Herzegovina': 'Sarajevo',\n",
       " 'Botswana': 'Gaborone',\n",
       " 'Bouvet Island': '',\n",
       " 'Brazil': 'Brasília',\n",
       " 'British Indian Ocean Territory': 'Diego Garcia',\n",
       " 'United States Minor Outlying Islands': '',\n",
       " 'Virgin Islands (British)': 'Road Town',\n",
       " 'Virgin Islands (U.S.)': 'Charlotte Amalie',\n",
       " 'Brunei Darussalam': 'Bandar Seri Begawan',\n",
       " 'Bulgaria': 'Sofia',\n",
       " 'Burkina Faso': 'Ouagadougou',\n",
       " 'Burundi': 'Bujumbura',\n",
       " 'Cambodia': 'Phnom Penh',\n",
       " 'Cameroon': 'Yaoundé',\n",
       " 'Canada': 'Ottawa',\n",
       " 'Cabo Verde': 'Praia',\n",
       " 'Cayman Islands': 'George Town',\n",
       " 'Central African Republic': 'Bangui',\n",
       " 'Chad': \"N'Djamena\",\n",
       " 'Chile': 'Santiago',\n",
       " 'China': 'Beijing',\n",
       " 'Christmas Island': 'Flying Fish Cove',\n",
       " 'Cocos (Keeling) Islands': 'West Island',\n",
       " 'Colombia': 'Bogotá',\n",
       " 'Comoros': 'Moroni',\n",
       " 'Congo': 'Brazzaville',\n",
       " 'Congo (Democratic Republic of the)': 'Kinshasa',\n",
       " 'Cook Islands': 'Avarua',\n",
       " 'Costa Rica': 'San José',\n",
       " 'Croatia': 'Zagreb',\n",
       " 'Cuba': 'Havana',\n",
       " 'Curaçao': 'Willemstad',\n",
       " 'Cyprus': 'Nicosia',\n",
       " 'Czech Republic': 'Prague',\n",
       " 'Denmark': 'Copenhagen',\n",
       " 'Djibouti': 'Djibouti',\n",
       " 'Dominica': 'Roseau',\n",
       " 'Dominican Republic': 'Santo Domingo',\n",
       " 'Ecuador': 'Quito',\n",
       " 'Egypt': 'Cairo',\n",
       " 'El Salvador': 'San Salvador',\n",
       " 'Equatorial Guinea': 'Malabo',\n",
       " 'Eritrea': 'Asmara',\n",
       " 'Estonia': 'Tallinn',\n",
       " 'Ethiopia': 'Addis Ababa',\n",
       " 'Falkland Islands (Malvinas)': 'Stanley',\n",
       " 'Faroe Islands': 'Tórshavn',\n",
       " 'Fiji': 'Suva',\n",
       " 'Finland': 'Helsinki',\n",
       " 'France': 'Paris',\n",
       " 'French Guiana': 'Cayenne',\n",
       " 'French Polynesia': 'Papeetē',\n",
       " 'French Southern Territories': 'Port-aux-Français',\n",
       " 'Gabon': 'Libreville',\n",
       " 'Gambia': 'Banjul',\n",
       " 'Georgia': 'Tbilisi',\n",
       " 'Germany': 'Berlin',\n",
       " 'Ghana': 'Accra',\n",
       " 'Gibraltar': 'Gibraltar',\n",
       " 'Greece': 'Athens',\n",
       " 'Greenland': 'Nuuk',\n",
       " 'Grenada': \"St. George's\",\n",
       " 'Guadeloupe': 'Basse-Terre',\n",
       " 'Guam': 'Hagåtña',\n",
       " 'Guatemala': 'Guatemala City',\n",
       " 'Guernsey': 'St. Peter Port',\n",
       " 'Guinea': 'Conakry',\n",
       " 'Guinea-Bissau': 'Bissau',\n",
       " 'Guyana': 'Georgetown',\n",
       " 'Haiti': 'Port-au-Prince',\n",
       " 'Heard Island and McDonald Islands': '',\n",
       " 'Holy See': 'Rome',\n",
       " 'Honduras': 'Tegucigalpa',\n",
       " 'Hong Kong': 'City of Victoria',\n",
       " 'Hungary': 'Budapest',\n",
       " 'Iceland': 'Reykjavík',\n",
       " 'India': 'New Delhi',\n",
       " 'Indonesia': 'Jakarta',\n",
       " \"Côte d'Ivoire\": 'Yamoussoukro',\n",
       " 'Iran (Islamic Republic of)': 'Tehran',\n",
       " 'Iraq': 'Baghdad',\n",
       " 'Ireland': 'Dublin',\n",
       " 'Isle of Man': 'Douglas',\n",
       " 'Israel': 'Jerusalem',\n",
       " 'Italy': 'Rome',\n",
       " 'Jamaica': 'Kingston',\n",
       " 'Japan': 'Tokyo',\n",
       " 'Jersey': 'Saint Helier',\n",
       " 'Jordan': 'Amman',\n",
       " 'Kazakhstan': 'Astana',\n",
       " 'Kenya': 'Nairobi',\n",
       " 'Kiribati': 'South Tarawa',\n",
       " 'Kuwait': 'Kuwait City',\n",
       " 'Kyrgyzstan': 'Bishkek',\n",
       " \"Lao People's Democratic Republic\": 'Vientiane',\n",
       " 'Latvia': 'Riga',\n",
       " 'Lebanon': 'Beirut',\n",
       " 'Lesotho': 'Maseru',\n",
       " 'Liberia': 'Monrovia',\n",
       " 'Libya': 'Tripoli',\n",
       " 'Liechtenstein': 'Vaduz',\n",
       " 'Lithuania': 'Vilnius',\n",
       " 'Luxembourg': 'Luxembourg',\n",
       " 'Macao': '',\n",
       " 'Macedonia (the former Yugoslav Republic of)': 'Skopje',\n",
       " 'Madagascar': 'Antananarivo',\n",
       " 'Malawi': 'Lilongwe',\n",
       " 'Malaysia': 'Kuala Lumpur',\n",
       " 'Maldives': 'Malé',\n",
       " 'Mali': 'Bamako',\n",
       " 'Malta': 'Valletta',\n",
       " 'Marshall Islands': 'Majuro',\n",
       " 'Martinique': 'Fort-de-France',\n",
       " 'Mauritania': 'Nouakchott',\n",
       " 'Mauritius': 'Port Louis',\n",
       " 'Mayotte': 'Mamoudzou',\n",
       " 'Mexico': 'Mexico City',\n",
       " 'Micronesia (Federated States of)': 'Palikir',\n",
       " 'Moldova (Republic of)': 'Chișinău',\n",
       " 'Monaco': 'Monaco',\n",
       " 'Mongolia': 'Ulan Bator',\n",
       " 'Montenegro': 'Podgorica',\n",
       " 'Montserrat': 'Plymouth',\n",
       " 'Morocco': 'Rabat',\n",
       " 'Mozambique': 'Maputo',\n",
       " 'Myanmar': 'Naypyidaw',\n",
       " 'Namibia': 'Windhoek',\n",
       " 'Nauru': 'Yaren',\n",
       " 'Nepal': 'Kathmandu',\n",
       " 'Netherlands': 'Amsterdam',\n",
       " 'New Caledonia': 'Nouméa',\n",
       " 'New Zealand': 'Wellington',\n",
       " 'Nicaragua': 'Managua',\n",
       " 'Niger': 'Niamey',\n",
       " 'Nigeria': 'Abuja',\n",
       " 'Niue': 'Alofi',\n",
       " 'Norfolk Island': 'Kingston',\n",
       " \"Korea (Democratic People's Republic of)\": 'Pyongyang',\n",
       " 'Northern Mariana Islands': 'Saipan',\n",
       " 'Norway': 'Oslo',\n",
       " 'Oman': 'Muscat',\n",
       " 'Pakistan': 'Islamabad',\n",
       " 'Palau': 'Ngerulmud',\n",
       " 'Palestine, State of': 'Ramallah',\n",
       " 'Panama': 'Panama City',\n",
       " 'Papua New Guinea': 'Port Moresby',\n",
       " 'Paraguay': 'Asunción',\n",
       " 'Peru': 'Lima',\n",
       " 'Philippines': 'Manila',\n",
       " 'Pitcairn': 'Adamstown',\n",
       " 'Poland': 'Warsaw',\n",
       " 'Portugal': 'Lisbon',\n",
       " 'Puerto Rico': 'San Juan',\n",
       " 'Qatar': 'Doha',\n",
       " 'Republic of Kosovo': 'Pristina',\n",
       " 'Réunion': 'Saint-Denis',\n",
       " 'Romania': 'Bucharest',\n",
       " 'Russian Federation': 'Moscow',\n",
       " 'Rwanda': 'Kigali',\n",
       " 'Saint Barthélemy': 'Gustavia',\n",
       " 'Saint Helena, Ascension and Tristan da Cunha': 'Jamestown',\n",
       " 'Saint Kitts and Nevis': 'Basseterre',\n",
       " 'Saint Lucia': 'Castries',\n",
       " 'Saint Martin (French part)': 'Marigot',\n",
       " 'Saint Pierre and Miquelon': 'Saint-Pierre',\n",
       " 'Saint Vincent and the Grenadines': 'Kingstown',\n",
       " 'Samoa': 'Apia',\n",
       " 'San Marino': 'City of San Marino',\n",
       " 'Sao Tome and Principe': 'São Tomé',\n",
       " 'Saudi Arabia': 'Riyadh',\n",
       " 'Senegal': 'Dakar',\n",
       " 'Serbia': 'Belgrade',\n",
       " 'Seychelles': 'Victoria',\n",
       " 'Sierra Leone': 'Freetown',\n",
       " 'Singapore': 'Singapore',\n",
       " 'Sint Maarten (Dutch part)': 'Philipsburg',\n",
       " 'Slovakia': 'Bratislava',\n",
       " 'Slovenia': 'Ljubljana',\n",
       " 'Solomon Islands': 'Honiara',\n",
       " 'Somalia': 'Mogadishu',\n",
       " 'South Africa': 'Pretoria',\n",
       " 'South Georgia and the South Sandwich Islands': 'King Edward Point',\n",
       " 'Korea (Republic of)': 'Seoul',\n",
       " 'South Sudan': 'Juba',\n",
       " 'Spain': 'Madrid',\n",
       " 'Sri Lanka': 'Colombo',\n",
       " 'Sudan': 'Khartoum',\n",
       " 'Suriname': 'Paramaribo',\n",
       " 'Svalbard and Jan Mayen': 'Longyearbyen',\n",
       " 'Swaziland': 'Lobamba',\n",
       " 'Sweden': 'Stockholm',\n",
       " 'Switzerland': 'Bern',\n",
       " 'Syrian Arab Republic': 'Damascus',\n",
       " 'Taiwan': 'Taipei',\n",
       " 'Tajikistan': 'Dushanbe',\n",
       " 'Tanzania, United Republic of': 'Dodoma',\n",
       " 'Thailand': 'Bangkok',\n",
       " 'Timor-Leste': 'Dili',\n",
       " 'Togo': 'Lomé',\n",
       " 'Tokelau': 'Fakaofo',\n",
       " 'Tonga': \"Nuku'alofa\",\n",
       " 'Trinidad and Tobago': 'Port of Spain',\n",
       " 'Tunisia': 'Tunis',\n",
       " 'Turkey': 'Ankara',\n",
       " 'Turkmenistan': 'Ashgabat',\n",
       " 'Turks and Caicos Islands': 'Cockburn Town',\n",
       " 'Tuvalu': 'Funafuti',\n",
       " 'Uganda': 'Kampala',\n",
       " 'Ukraine': 'Kiev',\n",
       " 'United Arab Emirates': 'Abu Dhabi',\n",
       " 'United Kingdom of Great Britain and Northern Ireland': 'London',\n",
       " 'United States of America': 'Washington, D.C.',\n",
       " 'Uruguay': 'Montevideo',\n",
       " 'Uzbekistan': 'Tashkent',\n",
       " 'Vanuatu': 'Port Vila',\n",
       " 'Venezuela (Bolivarian Republic of)': 'Caracas',\n",
       " 'Viet Nam': 'Hanoi',\n",
       " 'Wallis and Futuna': 'Mata-Utu',\n",
       " 'Western Sahara': 'El Aaiún',\n",
       " 'Yemen': \"Sana'a\",\n",
       " 'Zambia': 'Lusaka',\n",
       " 'Zimbabwe': 'Harare'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAPITALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text) # CAPITALS 는 dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Span extension attribute 'capital' with the getter get_capital\n",
    "Span.set_extension('capital', getter=get_capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace.\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대규모의 Doc을 전달할 때는 pipe 메소드를 사용하자. <br>단 pipe 메소드는 생성자를 반환하므로, list(nlp.pipe(LOTS_OF_TEXTS)) 형태로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('This is a text', {'id':1, 'page_number':15}),\n",
    "    ('And another text', {'id':2, 'page_number':16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension('id', default=None, force=True)\n",
    "Doc.set_extension('page_number', default=None,  force=True)\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nlp.make_doc` to just turn a text into a `Doc` object (just for tokenizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Please help me!!!\", He said. \"Please!\" '"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"\"\" \"Please help me!!!\", He said. \"Please!\" \"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Please help me!!!\", He said. \"Please!\" \n"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises : Processing streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spacy/exercises/tweets.json\") as f:\n",
    "    TEXT= json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['McDonalds is my favorite restaurant.',\n",
       " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
       " 'People really still eat McDonalds :(',\n",
       " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
       " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
       " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
       " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(TEXT):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) (WANT, McRib) (This morning,)\n"
     ]
    }
   ],
   "source": [
    "docs = list(nlp.pipe(TEXT))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[David Bowie, Angela Merkel, Lady Gaga]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = English()\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "#patterns = [nlp(person) for person in people]\n",
    "patterns = list(nlp.pipe(people))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises : Processing data with context\n",
    "\n",
    "In this exercise, you'll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of `[text, context]` examples is available as the variable `DATA`. The texts are quotes from famous books, and the contexts dictionaries with the keys `'author'` and `'book'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spacy/exercises/bookquotes.json') as f:\n",
    "    DATA = json.loads(f.read())\n",
    "    \n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
       " {'author': 'Franz Kafka', 'book': 'Metamorphosis'}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension('author', default=None)\n",
    "Doc.set_extension('book', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " - 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " - 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " - 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " - 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " - '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " - 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    print(doc.text, \"\\n\", \"- '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises : Selective processing\n",
    "\n",
    "In this exercise, you'll use the `nlp.make_doc` and `nlp.disable_pipes` methods to only run selected components when processing a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes('tagger','parser'):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
